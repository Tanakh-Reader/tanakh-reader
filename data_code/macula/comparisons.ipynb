{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_LEXICON_PATH = 'STEP Data Source/step/step-core-data/src/main/resources/com/tyndalehouse/step/core/data/create/lexicon/lexicon_hebrew.txt'\n",
    "STEP_CORPORA_PATH = 'STEP Data Source/STEPBible-Data'\n",
    "\n",
    "STEP_CORPORA_PREFIX = 'TOTHT'\n",
    "# STEP_CORPORA_HEADER = ['Ref in Heb', 'Eng ref', 'Pointed', 'Accented', 'Morphology', 'Extended Strongs']\n",
    "\n",
    "HEB_REF_ATTR = 'hebrewRef'\n",
    "ENG_REF_ATTR = 'englishRef'\n",
    "TEXT_ATTR = 'text'\n",
    "TEXT_QERE_ATTR = 'textQere'\n",
    "TRAILER_ATTR = 'trailer'\n",
    "GLOSS_ATTR = 'gloss'\n",
    "SENSE_GLOSS_ATTR = 'senseGloss'\n",
    "MORPH_ATTR = 'morph'\n",
    "STRONGS_ATTR = 'strongs'\n",
    "TRAILER_STRONGS_ATTR = 'trailerStrongs'\n",
    "\n",
    "STEP_CORPORA_HEADER = [HEB_REF_ATTR, ENG_REF_ATTR, TEXT_ATTR, TEXT_QERE_ATTR, TRAILER_ATTR, GLOSS_ATTR, SENSE_GLOSS_ATTR, MORPH_ATTR, STRONGS_ATTR, TRAILER_STRONGS_ATTR]\n",
    "\n",
    "STEP_DATA_DEST = 'STEP Data Destination'\n",
    "STEP_CORPUS = 'TOTHT.csv'\n",
    "STEP_CORPUS_WITH_QERE = 'TOTHT-with-qere.csv'\n",
    "\n",
    "MACULA_CORPUS = 'macula-hebrew.tsv'\n",
    "ETCBC_CORPUS = 'word.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepBibleHebrewDataProcessor:\n",
    "\n",
    "    trailers = ['׃', 'פ', '׀', 'ס', ' פ', '׆', '־']\n",
    "    # maqef = '־'\n",
    "\n",
    "    def __init__(self, corpora_files_path, lexicon_file_path):\n",
    "        \n",
    "        self.lexicon_file_path = lexicon_file_path\n",
    "        self.corpora_files_path = corpora_files_path\n",
    "        self.corpora_files_dict = self.get_corpora_dict()\n",
    "\n",
    "    def get_corpora_dict(self):\n",
    "\n",
    "        # Assuming file format 'TOTHT Gen-Deu - Translators OT Hebrew Tagged text - STEPBible.org CC BY.txt'\n",
    "        corpora_dict = {\n",
    "            'Gen': '',\n",
    "            'Jos': '',\n",
    "            'Job': '',\n",
    "            'Isa': ''\n",
    "        }\n",
    "\n",
    "        # Assign the files to the references in the dictionary.\n",
    "        for file in os.listdir(self.corpora_files_path):\n",
    "\n",
    "            if STEP_CORPORA_PREFIX in file:\n",
    "\n",
    "                for ref in corpora_dict.keys():\n",
    "\n",
    "                    if ref in file:\n",
    "\n",
    "                        corpora_dict[ref] = file\n",
    "\n",
    "        return corpora_dict\n",
    "\n",
    "    # Write all corpora files into a single corpus csv.\n",
    "    def write_corpora_data(self, with_qere=False):\n",
    "\n",
    "        rows = []   \n",
    "\n",
    "        for ref, file in self.corpora_files_dict.items():\n",
    "\n",
    "            file_path = os.path.join(STEP_CORPORA_PATH, file)\n",
    "            # Track when we've arrived at the Hebrew content in the file. \n",
    "            atData = False\n",
    "        \n",
    "            with open(file_path) as f:\n",
    "\n",
    "                lines = [line.rstrip('\\n') for line in f]\n",
    "                for line_i, line in enumerate(lines):\n",
    "\n",
    "                    row = line.split('\\t')\n",
    "\n",
    "                    # E.g., \"Gen.1.1-01\tGen.1.1-01\tבְּרֵאשִׁית\tבְּ/רֵאשִׁ֖ית\tHR/Ncfsa\tH9003=ב=in/H7225=רֵאשִׁית=first_§1_beginning\"\n",
    "                    if len(row) > 1 and ref in row[0]:\n",
    "                        atData = True\n",
    "\n",
    "                    if atData:\n",
    "                        \n",
    "                        # Don't include qere data.\n",
    "                        if len(row) == 6 and '.Q' not in row[0]:\n",
    "                            \n",
    "                            try:\n",
    "                                words = row[3].split('/')\n",
    "                                morph_codes = row[4].split('/')\n",
    "                                strongs_data = row[5].split('/')\n",
    "\n",
    "                            except Exception as e: \n",
    "                                print(e, line_i, row)\n",
    "\n",
    "                            word_count = 0\n",
    "\n",
    "                            data = {}\n",
    "\n",
    "                            for i, word in enumerate(words): \n",
    "\n",
    "                                if word == '':\n",
    "                                    continue\n",
    "\n",
    "                                try:\n",
    "\n",
    "                                    _strongs_data = strongs_data[i].split('=')\n",
    "                                    strongs_number = _strongs_data[0]\n",
    "                                    \n",
    "                                    gloss_data = _strongs_data[-1].split('_')\n",
    "                                    gloss = gloss_data[0]\n",
    "                                    sense_gloss = None \n",
    "                                    if len(gloss_data) == 2:\n",
    "                                        sense_gloss = gloss_data[1]\n",
    "                                    elif len(gloss_data) == 3:\n",
    "                                        sense_gloss = gloss_data[1] + '.' + gloss_data[2]\n",
    "                                \n",
    "                                except Exception as e: \n",
    "                                    print(e, line_i, i, word, row, words, morph_codes)\n",
    "\n",
    "                                # Faulty data:\n",
    "                                # 2Ki.7.15-14.K\t2Ki.7.15-14k\tבְּהֵחָפְזָם\tבְּ/הֵ/חָפְזָ/ם\tHR/VNcc/Sp3mp\tH9003=ב=in/H9009#1=ה=the/H2648=חָפַז=to hurry/H9048=Sp3m=they\n",
    "                                if strongs_number == 'H9009#1':\n",
    "                                    continue\n",
    "\n",
    "                                if word in self.trailers:\n",
    "\n",
    "                                    data[TRAILER_ATTR] += word \n",
    "                                    data[TRAILER_STRONGS_ATTR] = strongs_number\n",
    "\n",
    "                                else:\n",
    "\n",
    "                                    if data.get(TEXT_ATTR) != None:\n",
    "                                        rows.append(data)\n",
    "\n",
    "                                    data = {}\n",
    "\n",
    "                                    try:\n",
    "\n",
    "                                        morph = morph_codes[word_count] if len(words) > len(morph_codes) else morph_codes[i]\n",
    "\n",
    "                                        data[HEB_REF_ATTR] = row[0]\n",
    "                                        data[ENG_REF_ATTR] = row[1]\n",
    "                                        data[TEXT_ATTR] = word \n",
    "                                        data[TRAILER_ATTR] = '' if i != len(words) - 1 else ' '\n",
    "                                        data[GLOSS_ATTR] = gloss \n",
    "                                        data[SENSE_GLOSS_ATTR] = sense_gloss\n",
    "                                        data[MORPH_ATTR] = morph\n",
    "                                        data[STRONGS_ATTR] = strongs_number\n",
    "                                        data[TRAILER_STRONGS_ATTR] = None\n",
    "\n",
    "                                        if with_qere and '.K' in row[0]:\n",
    "                                            qere_data = lines[i+1].split('\\t')\n",
    "\n",
    "                                            # TODO data[]\n",
    "\n",
    "                                    except Exception as e: \n",
    "                                        print(e, line_i, i, word, row)\n",
    "\n",
    "                                    word_count += 1\n",
    "\n",
    "                            # if with_qere and '.K' in row[0]:\n",
    "                            #     data[]\n",
    "                            \n",
    "                            rows.append(data)\n",
    "\n",
    "            print('Complete: ' + file)\n",
    "\n",
    "        # Write the data.\n",
    "        df = pd.DataFrame(rows)\n",
    "        write_file = STEP_CORPUS_WITH_QERE if with_qere else STEP_CORPUS\n",
    "        save_path = os.path.join(STEP_DATA_DEST, write_file)\n",
    "        df.to_csv(save_path, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: TOTHT Gen-Deu - Translators OT Hebrew Tagged text - STEPBible.org CC BY.txt\n",
      "Complete: TOTHT Jos-Est - Translators OT Hebrew Tagged text - STEPBible.org CC BY.txt\n",
      "Complete: TOTHT Job-Sng - Translators OT Hebrew Tagged text - STEPBible.org CC BY.txt\n",
      "Complete: TOTHT Isa-Mal - Translators OT Hebrew Tagged text - STEPBible.org CC BY.txt\n"
     ]
    }
   ],
   "source": [
    "# a = StepBibleHebrewDataProcessor(STEP_CORPORA_PATH, STEP_LEXICON_PATH)\n",
    "# a.write_corpora_data()\n",
    "# print(a.corpora_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hebStripped(word):\n",
    "\n",
    "    normalized = unicodedata.normalize('NFKD', word)\n",
    "\n",
    "    return ''.join([c for c in normalized if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFileParser:\n",
    "\n",
    "    og_crawl_depth = 1\n",
    "    # According to index == case.\n",
    "    word_cases = [\"Same\", \"Dif markings\", \"Dif spelling\", \"Dif word, next same\", \"Dif word, next not same\"]\n",
    "\n",
    "    def __init__(self, file:str, word_col:str, ref_col:str, name:str):\n",
    "\n",
    "        self.df = pd.read_csv(\n",
    "            file, \n",
    "            sep=self.__get_sep(file), \n",
    "            na_filter=False,\n",
    "            encoding='utf-8',\n",
    "            usecols=[word_col, ref_col]\n",
    "            ).astype(str)\n",
    "        self.words = self.df[word_col].to_list()\n",
    "        self.refs = self.df[ref_col].to_list()\n",
    "        self.words_output = []\n",
    "        self.refs_output = []\n",
    "        self.cases_output = []\n",
    "        self.i = 0\n",
    "        self.crawl_depth = self.og_crawl_depth\n",
    "        self.length = len(self.words)\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "    def __get_sep(self, file:str) -> str:\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            return ','\n",
    "        elif file.endswith('.tsv'):\n",
    "            return '\\t'\n",
    "        # TODO raise error\n",
    "\n",
    "\n",
    "    def word(self, i:int=None) -> str:\n",
    "\n",
    "        if i:\n",
    "            return self.words[i]\n",
    "\n",
    "        return self.words[self.i]\n",
    "\n",
    "\n",
    "    def ref(self, i:int=None) -> str:\n",
    "\n",
    "        if i:\n",
    "            return self.refs[i]\n",
    "\n",
    "        return self.refs[self.i]\n",
    "\n",
    "\n",
    "    def reset_crawl_depth(self):\n",
    "\n",
    "        self.crawl_depth = self.og_crawl_depth\n",
    "\n",
    "\n",
    "    def update_output_lists(self, values:list):\n",
    "        \n",
    "        self.words_output.append(values[0])\n",
    "        self.refs_output.append(values[1])\n",
    "        self.cases_output.append(values[2])\n",
    "\n",
    "\n",
    "    def word_comparison(self, other_wfp:'WordFileParser', new_index:int=0) -> int:\n",
    "        \n",
    "        other_index = max(other_wfp.i, new_index)\n",
    "        word_a = self.word()\n",
    "        word_b = other_wfp.word(other_index)\n",
    "\n",
    "        if word_a == word_b:\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            word_a_cons = hebStripped(word_a)\n",
    "            word_b_cons = hebStripped(word_b)\n",
    "\n",
    "            if word_a_cons == word_b_cons:\n",
    "                return 1\n",
    "\n",
    "            elif 0 in [len(word_a_cons), len(word_b_cons)]:\n",
    "                return 4\n",
    "\n",
    "            elif len(word_a_cons) > 1 and len(word_b_cons) > 1 and word_a_cons[0] == word_b_cons[0]:\n",
    "\n",
    "                if word_a_cons[-1] == word_b_cons[-1] or len(word_a_cons) == len(word_b_cons):\n",
    "                    return 1\n",
    "                \n",
    "                else:\n",
    "                    return 2\n",
    "\n",
    "            elif self.i + 1 < self.length and other_wfp.i + 1 < other_wfp.length \\\n",
    "            and hebStripped(self.word(self.i+1)) == hebStripped(other_wfp.word(other_index+1)):\n",
    "                return 3\n",
    "\n",
    "            else:\n",
    "                return 4\n",
    "\n",
    "\n",
    "    def crawl(self, other_wfp:'WordFileParser', again=False):\n",
    "        \n",
    "        depth = 0\n",
    "        runner_index = other_wfp.i + 1\n",
    "\n",
    "        while depth < self.crawl_depth and runner_index < other_wfp.length:\n",
    "\n",
    "            comp = self.word_comparison(other_wfp, new_index=runner_index)\n",
    "\n",
    "            if comp == 0 or (again and comp in [1,2,3]):\n",
    "                self.update_comparisons(other_wfp, comp, runner_index)\n",
    "                return True\n",
    "                \n",
    "            runner_index += 1\n",
    "            depth += 1\n",
    "        \n",
    "        return False\n",
    "\n",
    "            \n",
    "    def update_comparisons(self, other_wfp:'WordFileParser', comp:int, new_index:int=0):\n",
    "\n",
    "        while other_wfp.i < new_index:\n",
    "            other_wfp.update_output_lists([other_wfp.word(), other_wfp.ref(), 4])\n",
    "            self.update_output_lists(['NA', self.ref(), 4])\n",
    "            other_wfp.i += 1\n",
    "            \n",
    "        if comp in range(4):\n",
    "\n",
    "            other_wfp.update_output_lists([other_wfp.word(), other_wfp.ref(), comp])\n",
    "            self.update_output_lists([self.word(), self.ref(), comp])\n",
    "            self.i += 1\n",
    "            other_wfp.i += 1\n",
    "\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def add_row(self, other_wfp:'WordFileParser'):\n",
    "        \n",
    "        other_wfp.update_output_lists([other_wfp.word(), other_wfp.ref(), 5])\n",
    "        self.update_output_lists([self.word(), self.ref(), 5])\n",
    "        self.i += 1\n",
    "        other_wfp.i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_file = os.path.join(STEP_DATA_DEST, STEP_CORPUS)\n",
    "step_word_col = 'text'\n",
    "step_ref_col = 'hebrewRef'\n",
    "\n",
    "macula_file = os.path.join(\"\", MACULA_CORPUS)\n",
    "macula_word_col = 'text'\n",
    "macula_ref_col = 'ref'\n",
    "\n",
    "etcbc_file = os.path.join(\"\", ETCBC_CORPUS)\n",
    "etcbc_word_col = 'text'\n",
    "etcbc_ref_col = 'vsIdBHS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data(wfp1:WordFileParser, wfp2:WordFileParser):\n",
    "    \n",
    "    while wfp1.i < wfp1.length and wfp2.i < wfp2.length:\n",
    "        \n",
    "        comp = wfp1.word_comparison(wfp2)\n",
    "        \n",
    "        if not wfp1.update_comparisons(wfp2, comp):\n",
    "\n",
    "            while wfp1.crawl_depth <= 3:\n",
    "                if wfp1.crawl(wfp2):\n",
    "                    break\n",
    "                elif wfp2.crawl(wfp1):\n",
    "                    break\n",
    "                elif wfp1.crawl(wfp2, again=True):\n",
    "                    break\n",
    "                elif wfp2.crawl(wfp1, again=True):\n",
    "                    break\n",
    "                wfp1.crawl_depth += 1\n",
    "                wfp2.crawl_depth += 1\n",
    "            \n",
    "            else:\n",
    "                wfp1.add_row(wfp2)\n",
    "                # print(\"ERROR\", wfp1.crawl_dist, wfp1.i, wfp1.word(), wfp2.i, wfp2.word())\n",
    "                # return table\n",
    "\n",
    "            wfp1.reset_crawl_depth()\n",
    "            wfp2.reset_crawl_depth()\n",
    "\n",
    "        if wfp1.i % 50000 < 1:\n",
    "            print(wfp1.i, wfp1.word(), wfp2.i, wfp2.word())\n",
    "\n",
    "    table = {\n",
    "        f\"{wfp1.name}Ref\": wfp1.refs_output,\n",
    "        f\"{wfp1.name}Text\": wfp1.words_output,\n",
    "        f\"{wfp2.name}Text\": wfp2.words_output,\n",
    "        f\"{wfp2.name}Ref\": wfp2.refs_output,\n",
    "        \"code\": wfp2.cases_output,\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(table).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_wfp = WordFileParser(step_file, step_word_col, step_ref_col, 'step')\n",
    "macula_wfp = WordFileParser(macula_file, macula_word_col, macula_ref_col, 'macula')\n",
    "etcbc_wfp = WordFileParser(etcbc_file, etcbc_word_col, etcbc_ref_col, 'etcbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 בַּדִּים֙ 51272 לְ\n",
      "200000 עַבְדֹּ֑ו 202872 וַ\n"
     ]
    }
   ],
   "source": [
    "# df = compare_data(macula_wfp, step_wfp)\n",
    "# df = compare_data(step_wfp, macula_wfp)\n",
    "df = compare_data(etcbc_wfp, macula_wfp)\n",
    "# df = compare_data(step_wfp, etcbc_wfp)\n",
    "\n",
    "write_file = 'comp10.csv'\n",
    "df.to_csv(write_file, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nw_align(a, b, replace_func=lambda x, y: -1 if x != y else 0, insert=-1, delete=-1):\n",
    "    ZERO, LEFT, UP, DIAGONAL = 0, 1, 2, 3\n",
    "    len_a, len_b = len(a), len(b)\n",
    "    matrix = [[(0, ZERO) for x in range(len_b + 1)] for y in range(len_a + 1)]\n",
    "    for i in range(len_a + 1):\n",
    "        matrix[i][0] = (insert * i, UP)\n",
    "    for j in range(len_b + 1):\n",
    "        matrix[0][j] = (delete * j, LEFT)\n",
    "    for i in range(1, len_a + 1):\n",
    "        for j in range(1, len_b + 1):\n",
    "            replace = replace_func(a[i - 1], b[j - 1])\n",
    "            matrix[i][j] = max(\n",
    "                [\n",
    "                    (matrix[i - 1][j - 1][0] + replace, DIAGONAL),\n",
    "                    (matrix[i][j - 1][0] + insert, LEFT),\n",
    "                    (matrix[i - 1][j][0] + delete, UP),\n",
    "                ]\n",
    "            )\n",
    "    i, j = len_a, len_b\n",
    "    alignment = []\n",
    "    while (i, j) != (0, 0):\n",
    "        if matrix[i][j][1] == DIAGONAL:\n",
    "            alignment.insert(0, (a[i - 1], b[j - 1]))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif matrix[i][j][1] == LEFT:\n",
    "            alignment.insert(0, (None, b[j - 1]))\n",
    "            j -= 1\n",
    "        else:  # UP\n",
    "            alignment.insert(0, (a[i - 1], None))\n",
    "            i -= 1\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def replace_func(a, b):\n",
    "    if a == b[0]:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def align_text(a, b):\n",
    "    result = nw_align(a, b, replace_func=replace_func)\n",
    "    for x, y in result:\n",
    "        if y is None:\n",
    "            yield None\n",
    "        elif x:\n",
    "            yield y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 1000\n",
    "aligned = nw_align(macula_wfp.words[a:a+b], etcbc_wfp.words[a:a+b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('נּוּ', 'אִשָּׁ֖ה'), ('יַ֜יִן', 'מֵ'), ('גַּם', 'אֶ֥רֶץ'), ('הַ', 'מִצְרָֽיִם'), ('לַּ֗יְלָה', 'וַֽ'), ('וּ', 'יְהִי֙'), ('בֹ֨אִי֙', 'בָּ'), ('שִׁכְבִ֣י', ''), ('עִמּ֔', 'עֵ֣ת'), ('וֹ', 'הַ'), ('וּ', 'הִ֔וא'), ('נְחַיֶּ֥ה', 'וַ'), ('מֵ', 'יֹּ֣אמֶר'), ('אָבִ֖י', 'אֲבִימֶ֗לֶךְ'), ('נוּ', 'וּ'), ('זָֽרַע', 'פִיכֹל֙'), ('וַ', 'שַׂר'), ('תַּשְׁקֶ֜יןָ', 'צְבָאֹ֔ו'), ('גַּ֣ם', 'אֶל'), ('בַּ', 'אַבְרָהָ֖ם'), ('', 'לֵ'), ('לַּ֧יְלָה', 'אמֹ֑ר'), ('הַ', 'אֱלֹהִ֣ים'), ('ה֛וּא', 'עִמְּךָ֔'), ('אֶת', 'בְּ'), ('אֲבִי', 'כֹ֥ל'), ('הֶ֖ן', 'אֲשֶׁר'), ('יָ֑יִן', 'אַתָּ֖ה'), ('וַ', 'עֹשֶֽׂה'), ('תָּ֤קָם', 'וְ'), ('הַ', 'עַתָּ֗ה'), ('צְּעִירָה֙', 'הִשָּׁ֨בְעָה'), ('וַ', 'לִּ֤י'), ('תִּשְׁכַּ֣ב', 'בֵֽ'), ('עִמּ֔', 'אלֹהִים֙'), ('וֹ', 'הֵ֔נָּה'), ('וְ', 'אִם'), ('לֹֽא', 'תִּשְׁקֹ֣ר'), ('יָדַ֥ע', 'לִ֔י'), ('בְּ', 'וּ'), ('שִׁכְבָ֖', 'לְ'), ('הּ', 'נִינִ֖י'), ('וּ', 'וּ'), (None, 'לְ'), ('בְ', 'נֶכְדִּ֑י'), ('קֻמָֽ', 'כַּ'), ('הּ', ''), ('וַֽ', 'חֶ֜סֶד'), ('תַּהֲרֶ֛יןָ', 'אֲשֶׁר'), ('שְׁתֵּ֥י', 'עָשִׂ֤יתִי'), ('בְנֽוֹת', 'עִמְּךָ֙'), ('ל֖וֹט', 'תַּעֲשֶׂ֣ה'), ('מֵ', 'עִמָּדִ֔י'), ('אֲבִי', 'וְ'), ('הֶֽן', 'עִם'), ('וַ', 'הָ'), ('תֵּ֤לֶד', 'אָ֖רֶץ'), ('הַ', 'אֲשֶׁר'), ('בְּכִירָה֙', 'גַּ֥רְתָּה'), ('בֵּ֔ן', 'בָּֽהּ'), ('וַ', 'וַ'), (None, 'יֹּ֨אמֶר֙'), ('תִּקְרָ֥א', 'אַבְרָהָ֔ם'), ('שְׁמ֖', 'אָנֹכִ֖י'), ('וֹ', 'אִשָּׁבֵֽעַ'), ('מוֹאָ֑ב', 'וְ'), ('ה֥וּא', 'הֹוכִ֥חַ'), ('אֲבִֽי', 'אַבְרָהָ֖ם'), ('מוֹאָ֖ב', 'אֶת'), ('עַד', 'אֲבִימֶ֑לֶךְ'), ('הַ', 'עַל'), ('יּֽוֹם', 'אֹדֹות֙'), ('וְ', 'בְּאֵ֣ר'), ('הַ', 'הַ'), ('צְּעִירָ֤ה', 'מַּ֔יִם'), ('גַם', 'אֲשֶׁ֥ר'), ('הִוא֙', 'גָּזְל֖וּ'), ('יָ֣לְדָה', 'עַבְדֵ֥י'), ('בֵּ֔ן', 'אֲבִימֶֽלֶךְ'), ('וַ', 'וַ'), (None, 'יֹּ֣אמֶר'), (None, 'אֲבִימֶ֔לֶךְ'), (None, 'לֹ֣א'), (None, 'יָדַ֔עְתִּי'), ('תִּקְרָ֥א', 'מִ֥י'), ('שְׁמ֖', 'עָשָׂ֖ה'), ('וֹ', 'אֶת'), ('בֶּן', 'הַ'), ('עַמִּ֑י', 'דָּבָ֣ר'), ('ה֛וּא', 'הַ'), ('אֲבִ֥י', 'זֶּ֑ה'), ('בְנֵֽי', 'וְ'), ('עַמּ֖וֹן', 'גַם'), ('עַד', 'אַתָּ֞ה'), ('הַ', 'לֹא'), ('יּֽוֹם', 'הִגַּ֣דְתָּ'), ('וַ', 'לִּ֗י'), ('יִּסַּ֨ע', 'וְ'), ('מִ', 'גַ֧ם'), ('שָּׁ֤ם', 'אָנֹכִ֛י'), ('אַבְרָהָם֙', 'לֹ֥א'), ('אַ֣רְצָ', 'שָׁמַ֖עְתִּי'), ('ה', 'בִּלְתִּ֥י'), ('הַ', 'הַ'), ('נֶּ֔גֶב', 'יֹּֽום'), ('וַ', 'וַ'), ('יֵּ֥שֶׁב', 'יִּקַּ֤ח'), ('בֵּין', 'אַבְרָהָם֙'), ('קָדֵ֖שׁ', 'צֹ֣אן'), ('וּ', 'וּ'), ('בֵ֣ין', None), ('שׁ֑וּר', 'בָקָ֔ר'), ('וַ', 'וַ'), ('יָּ֖גָר', 'יִּתֵּ֖ן'), ('בִּ', 'לַ'), ('גְרָֽר', 'אֲבִימֶ֑לֶךְ'), ('וַ', 'וַ'), (None, 'יִּכְרְת֥וּ'), ('יֹּ֧אמֶר', 'שְׁנֵיהֶ֖ם'), ('אַבְרָהָ֛ם', 'בְּרִֽית'), ('אֶל', 'וַ'), ('שָׂרָ֥ה', 'יַּצֵּ֣ב'), ('אִשְׁתּ֖', 'אַבְרָהָ֗ם'), ('וֹ', 'אֶת'), ('אֲחֹ֣תִ', 'שֶׁ֛בַע'), ('י', 'כִּבְשֹׂ֥ת'), ('הִ֑וא', 'הַ'), ('וַ', 'צֹּ֖אן'), ('יִּשְׁלַ֗ח', 'לְ'), ('אֲבִימֶ֨לֶךְ֙', 'בַדְּהֶֽן'), ('מֶ֣לֶךְ', 'וַ'), ('גְּרָ֔ר', 'יֹּ֥אמֶר'), ('וַ', 'אֲבִימֶ֖לֶךְ'), ('יִּקַּ֖ח', 'אֶל'), ('אֶת', 'אַבְרָהָ֑ם'), ('שָׂרָֽה', 'מָ֣ה'), ('וַ', 'הֵ֗נָּה'), ('יָּבֹ֧א', 'שֶׁ֤בַע'), ('אֱלֹהִ֛ים', 'כְּבָשֹׂת֙'), ('אֶל', 'הָ'), ('אֲבִימֶ֖לֶךְ', 'אֵ֔לֶּה'), ('בַּ', 'אֲשֶׁ֥ר'), ('חֲל֣וֹם', 'הִצַּ֖בְתָּ'), ('הַ', 'לְ'), ('לָּ֑יְלָה', 'בַדָּֽנָה'), ('וַ', 'וַ'), (None, 'יֹּ֕אמֶר'), ('יֹּ֣אמֶר', 'כִּ֚י'), ('ל֗', 'אֶת'), ('וֹ', 'שֶׁ֣בַע'), ('הִנְּ', 'כְּבָשֹׂ֔ת'), ('ךָ֥', 'תִּקַּ֖ח'), ('מֵת֙', 'מִ'), ('עַל', 'יָּדִ֑י'), ('הָ', 'בַּ'), ('אִשָּׁ֣ה', 'עֲבוּר֙'), ('אֲשֶׁר', 'תִּֽהְיֶה'), ('לָקַ֔חְתָּ', 'לִּ֣י'), ('וְ', 'לְ'), ('הִ֖וא', 'עֵדָ֔ה'), ('בְּעֻ֥לַת', 'כִּ֥י'), ('בָּֽעַל', 'חָפַ֖רְתִּי'), ('וַ', 'אֶת'), ('אֲבִימֶ֕לֶךְ', 'הַ'), ('לֹ֥א', 'בְּאֵ֥ר'), ('קָרַ֖ב', 'הַ'), ('אֵלֶ֑י', 'זֹּֽאת'), ('הָ', 'עַל'), ('וַ', 'כֵּ֗ן'), ('יֹּאמַ֕ר', 'קָרָ֛א'), ('אֲדֹנָ֕', 'לַ'), ('י', ''), ('הֲ', 'מָּקֹ֥ום'), ('ג֥וֹי', 'הַ'), ('גַּם', 'ה֖וּא'), ('צַדִּ֖יק', 'בְּאֵ֣ר'), ('תַּהֲרֹֽג', 'שָׁ֑בַע'), ('הֲ', 'כִּ֛י'), ('לֹ֨א', 'שָׁ֥ם'), ('ה֤וּא', 'נִשְׁבְּע֖וּ'), ('אָֽמַר', 'שְׁנֵיהֶֽם'), ('לִ', 'וַ'), ('י֙', 'יִּכְרְת֥וּ'), ('אֲחֹ֣תִ', 'בְרִ֖ית'), ('י', 'בִּ'), ('הִ֔וא', 'בְאֵ֣ר'), ('וְ', 'שָׁ֑בַע'), ('הִֽיא', 'וַ'), ('גַם', 'יָּ֣קָם'), ('הִ֥וא', 'אֲבִימֶ֗לֶךְ'), ('אָֽמְרָ֖ה', 'וּ'), ('אָחִ֣', 'פִיכֹל֙'), ('י', 'שַׂר'), ('ה֑וּא', 'צְבָאֹ֔ו'), ('בְּ', 'וַ'), ('תָם', 'יָּשֻׁ֖בוּ'), ('לְבָבִ֛', 'אֶל'), ('י', 'אֶ֥רֶץ'), ('וּ', 'פְּלִשְׁתִּֽים'), ('בְ', 'וַ'), ('נִקְיֹ֥ן', 'יִּטַּ֥ע'), ('כַּפַּ֖', 'אֶ֖שֶׁל'), ('י', 'בִּ'), ('עָשִׂ֥יתִי', 'בְאֵ֣ר'), ('זֹֽאת', 'שָׁ֑בַע'), ('וַ', 'וַ'), ('יֹּאמֶר֩', 'יִּ֨קְרָא'), ('אֵלָ֨י', 'שָׁ֔ם'), ('ו', 'בְּ'), ('הָֽ', 'שֵׁ֥ם'), ('אֱלֹהִ֜ים', 'יְהוָ֖ה'), ('בַּ', 'אֵ֥ל'), ('חֲלֹ֗ם', 'עֹולָֽם'), ('גַּ֣ם', 'וַ'), ('אָנֹכִ֤י', 'יָּ֧גָר'), ('יָדַ֨עְתִּי֙', 'אַבְרָהָ֛ם'), ('כִּ֤י', 'בְּ'), ('בְ', 'אֶ֥רֶץ'), ('תָם', 'פְּלִשְׁתִּ֖ים'), ('לְבָבְ', 'יָמִ֥ים'), ('ךָ֙', 'רַבִּֽים'), ('עָשִׂ֣יתָ', 'וַ'), ('זֹּ֔את', 'יְהִ֗י'), ('וָ', 'אַחַר֙'), ('אֶחְשֹׂ֧ךְ', 'הַ'), ('גַּם', 'דְּבָרִ֣ים'), ('אָנֹכִ֛י', 'הָ'), ('אֽוֹתְ', 'אֵ֔לֶּה'), ('ךָ֖', 'וְ'), ('מֵ', 'הָ֣'), ('חֲטוֹ', 'אֱלֹהִ֔ים'), ('לִ֑', 'נִסָּ֖ה'), ('י', 'אֶת'), ('עַל', 'אַבְרָהָ֑ם'), ('כֵּ֥ן', 'וַ'), ('לֹא', 'יֹּ֣אמֶר'), ('נְתַתִּ֖י', 'אֵלָ֔יו'), ('ךָ', 'אַבְרָהָ֖ם'), ('לִ', 'וַ'), ('נְגֹּ֥עַ', 'יֹּ֥אמֶר'), ('אֵלֶֽי', 'הִנֵּֽנִי'), ('הָ', 'וַ'), ('וְ', 'יֹּ֡אמֶר'), ('עַתָּ֗ה', 'קַח'), ('הָשֵׁ֤ב', 'נָ֠א'), ('אֵֽשֶׁת', 'אֶת'), ('הָ', 'בִּנְךָ֙'), ('אִישׁ֙', 'אֶת'), ('כִּֽי', 'יְחִֽידְךָ֤'), ('נָבִ֣יא', 'אֲשֶׁר'), ('ה֔וּא', 'אָהַ֨בְתָּ֙'), ('וְ', 'אֶת'), ('יִתְפַּלֵּ֥ל', 'יִצְחָ֔ק'), ('בַּֽעַדְ', 'וְ'), ('ךָ֖', 'לֶךְ'), ('וֶֽ', 'לְךָ֔'), ('חְיֵ֑ה', 'אֶל'), ('וְ', 'אֶ֖רֶץ'), ('אִם', 'הַ'), ('אֵֽינְ', 'מֹּרִיָּ֑ה'), ('ךָ֣', 'וְ'), ('מֵשִׁ֗יב', 'הַעֲלֵ֤הוּ'), ('דַּ֚ע', 'שָׁם֙'), ('כִּי', 'לְ'), ('מ֣וֹת', 'עֹלָ֔ה'), ('תָּמ֔וּת', 'עַ֚ל'), ('אַתָּ֖ה', 'אַחַ֣ד'), ('וְ', 'הֶֽ'), ('כָל', 'הָרִ֔ים'), ('אֲשֶׁר', 'אֲשֶׁ֖ר'), ('לָֽ', 'אֹמַ֥ר'), ('ךְ', 'אֵלֶֽיךָ'), ('וַ', 'וַ'), ('יַּשְׁכֵּ֨ם', 'יַּשְׁכֵּ֨ם'), ('אֲבִימֶ֜לֶךְ', 'אַבְרָהָ֜ם'), ('בַּ', 'בַּ'), ('', ''), ('בֹּ֗קֶר', 'בֹּ֗קֶר'), ('וַ', None), ('יִּקְרָא֙', None), ('לְ', 'וַֽ'), ('כָל', 'יַּחֲבֹשׁ֙'), ('עֲבָדָ֔י', 'אֶת'), ('ו', 'חֲמֹרֹ֔ו'), ('וַ', 'וַ'), ('יְדַבֵּ֛ר', 'יִּקַּ֞ח'), ('אֶת', 'אֶת'), ('כָּל', 'שְׁנֵ֤י'), ('הַ', 'נְעָרָיו֙'), ('דְּבָרִ֥ים', 'אִתֹּ֔ו'), ('הָ', 'וְ'), ('אֵ֖לֶּה', 'אֵ֖ת'), ('בְּ', 'יִצְחָ֣ק'), ('אָזְנֵי', 'בְּנֹ֑ו'), ('הֶ֑ם', 'וַ'), ('וַ', 'יְבַקַּע֙'), ('יִּֽירְא֥וּ', 'עֲצֵ֣י'), ('הָ', 'עֹלָ֔ה'), ('אֲנָשִׁ֖ים', 'וַ'), ('מְאֹֽד', 'יָּ֣קָם'), ('וַ', 'וַ'), ('יִּקְרָ֨א', 'יֵּ֔לֶךְ'), ('אֲבִימֶ֜לֶךְ', 'אֶל'), ('לְ', 'הַ'), ('אַבְרָהָ֗ם', 'מָּקֹ֖ום'), ('וַ', 'אֲשֶׁר'), ('יֹּ֨אמֶר', 'אָֽמַר'), ('ל֜', 'לֹ֥ו'), ('וֹ', 'הָ'), ('מֶֽה', 'אֱלֹהִֽים'), ('עָשִׂ֤יתָ', 'בַּ'), ('לָּ֨', ''), ('נוּ֙', 'יֹּ֣ום'), ('וּ', 'הַ'), ('מֶֽה', 'שְּׁלִישִׁ֗י'), ('חָטָ֣אתִי', 'וַ'), ('לָ֔', 'יִּשָּׂ֨א'), ('ךְ', 'אַבְרָהָ֧ם'), ('כִּֽי', 'אֶת'), ('הֵבֵ֧אתָ', 'עֵינָ֛יו'), ('עָלַ֛', 'וַ'), ('י', 'יַּ֥רְא'), ('וְ', 'אֶת'), ('עַל', 'הַ'), ('מַמְלַכְתִּ֖', 'מָּקֹ֖ום'), ('י', 'מֵ'), ('חֲטָאָ֣ה', 'רָחֹֽק'), ('גְדֹלָ֑ה', 'וַ'), ('מַעֲשִׂים֙', 'יֹּ֨אמֶר'), ('אֲשֶׁ֣ר', 'אַבְרָהָ֜ם'), ('לֹא', 'אֶל'), ('יֵֽעָשׂ֔וּ', 'נְעָרָ֗יו'), ('עָשִׂ֖יתָ', 'שְׁבוּ'), ('עִמָּדִֽ', 'לָכֶ֥ם'), ('י', 'פֹּה֙'), ('וַ', 'עִֽם'), ('יֹּ֥אמֶר', 'הַ'), ('אֲבִימֶ֖לֶךְ', 'חֲמֹ֔ור'), ('אֶל', 'וַ'), ('אַבְרָהָ֑ם', 'אֲנִ֣י'), ('מָ֣ה', 'וְ'), ('רָאִ֔יתָ', 'הַ'), ('כִּ֥י', 'נַּ֔עַר'), ('עָשִׂ֖יתָ', 'נֵלְכָ֖ה'), ('אֶת', 'עַד'), ('הַ', 'כֹּ֑ה'), ('דָּבָ֥ר', 'וְ'), ('הַ', 'נִֽשְׁתַּחֲוֶ֖ה'), ('זֶּֽה', 'וְ'), ('וַ', 'נָשׁ֥וּבָה'), ('יֹּ֨אמֶר֙', 'אֲלֵיכֶֽם'), ('אַבְרָהָ֔ם', 'וַ'), ('כִּ֣י', 'יִּקַּ֨ח'), ('אָמַ֗רְתִּי', 'אַבְרָהָ֜ם'), ('רַ֚ק', 'אֶת'), ('אֵין', 'עֲצֵ֣י'), ('יִרְאַ֣ת', 'הָ'), ('אֱלֹהִ֔ים', 'עֹלָ֗ה'), ('בַּ', 'וַ'), ('', 'יָּ֨שֶׂם֙'), ('מָּק֖וֹם', 'עַל'), ('הַ', 'יִצְחָ֣ק'), ('זֶּ֑ה', 'בְּנֹ֔ו'), ('וַ', 'וַ'), ('הֲרָג֖וּ', 'יִּקַּ֣ח'), ('נִי', 'בְּ'), ('עַל', 'יָדֹ֔ו'), ('דְּבַ֥ר', 'אֶת'), ('אִשְׁתִּֽ', 'הָ'), ('י', 'אֵ֖שׁ'), ('וְ', 'וְ'), ('גַם', 'אֶת'), ('אָמְנָ֗ה', 'הַֽ'), ('אֲחֹתִ֤', 'מַּאֲכֶ֑לֶת'), ('י', 'וַ'), ('בַת', 'יֵּלְכ֥וּ'), ('אָבִ', 'שְׁנֵיהֶ֖ם'), ('י֙', 'יַחְדָּֽו'), ('הִ֔וא', 'וַ'), ('אַ֖ךְ', 'יֹּ֨אמֶר'), ('לֹ֣א', 'יִצְחָ֜ק'), ('בַת', 'אֶל'), ('אִמִּ֑', 'אַבְרָהָ֤ם'), ('י', 'אָבִיו֙'), ('וַ', 'וַ'), ('תְּהִי', None), ('לִ֖', None), ('י', None), ('לְ', 'יֹּ֣אמֶר'), ('אִשָּֽׁה', 'אָבִ֔י'), ('וַ', 'וַ'), ('יְהִ֞י', 'יֹּ֖אמֶר'), ('כַּ', 'הִנֶּ֣נִּֽי'), ('אֲשֶׁ֧ר', 'בְנִ֑י'), ('הִתְע֣וּ', 'וַ'), ('אֹתִ֗', 'יֹּ֗אמֶר'), ('י', 'הִנֵּ֤ה'), ('אֱלֹהִים֮', 'הָ'), ('מִ', 'אֵשׁ֙'), ('בֵּ֣ית', 'וְ'), ('אָבִ', 'הָ֣'), ('י֒', 'עֵצִ֔ים'), ('וָ', 'וְ'), ('אֹמַ֣ר', 'אַיֵּ֥ה'), ('לָ֔', 'הַ'), ('הּ', 'שֶּׂ֖ה'), ('זֶ֣ה', 'לְ'), ('חַסְדֵּ֔', 'עֹלָֽה'), ('ךְ', 'וַ'), ('אֲשֶׁ֥ר', 'יֹּ֨אמֶר֙'), ('תַּעֲשִׂ֖י', 'אַבְרָהָ֔ם'), ('עִמָּדִ֑', 'אֱלֹהִ֞ים'), ('י', 'יִרְאֶה'), ('אֶ֤ל', 'לֹּ֥ו'), ('כָּל', 'הַ'), ('הַ', 'שֶּׂ֛ה'), ('מָּקוֹם֙', 'לְ'), ('אֲשֶׁ֣ר', 'עֹלָ֖ה'), ('נָב֣וֹא', 'בְּנִ֑י'), ('שָׁ֔מָּ', 'וַ'), ('ה', 'יֵּלְכ֥וּ'), ('אִמְרִי', 'שְׁנֵיהֶ֖ם'), ('לִ֖', 'יַחְדָּֽו'), ('י', 'וַ'), ('אָחִ֥', 'יָּבֹ֗אוּ'), ('י', 'אֶֽל'), ('הֽוּא', 'הַ'), ('וַ', 'מָּקֹום֮'), ('יִּקַּ֨ח', 'אֲשֶׁ֣ר'), ('אֲבִימֶ֜לֶךְ', 'אָֽמַר'), ('צֹ֣אן', 'לֹ֣ו'), ('וּ', 'הָ'), ('בָקָ֗ר', 'אֱלֹהִים֒'), ('וַ', 'וַ'), ('עֲבָדִים֙', 'יִּ֨בֶן'), ('וּ', 'שָׁ֤ם'), ('שְׁפָחֹ֔ת', 'אַבְרָהָם֙'), ('וַ', 'אֶת'), ('יִּתֵּ֖ן', 'הַ'), ('לְ', 'מִּזְבֵּ֔חַ'), ('אַבְרָהָ֑ם', 'וַֽ'), ('וַ', 'יַּעֲרֹ֖ךְ'), ('יָּ֣שֶׁב', 'אֶת'), ('ל֔', 'הָ'), ('וֹ', 'עֵצִ֑ים'), ('אֵ֖ת', 'וַֽ'), ('שָׂרָ֥ה', 'יַּעֲקֹד֙'), ('אִשְׁתּֽ', 'אֶת'), ('וֹ', 'יִצְחָ֣ק'), ('וַ', 'בְּנֹ֔ו'), ('יֹּ֣אמֶר', 'וַ'), ('אֲבִימֶ֔לֶךְ', 'יָּ֤שֶׂם'), ('הִנֵּ֥ה', 'אֹתֹו֙'), ('אַרְצִ֖', 'עַל'), ('י', 'הַ'), ('לְ', 'מִּזְבֵּ֔חַ'), ('פָנֶ֑י', 'מִ'), ('ךָ', 'מַּ֖עַל'), ('בַּ', 'לָ'), ('', ''), ('טּ֥וֹב', 'עֵצִֽים'), ('בְּ', 'וַ'), ('עֵינֶ֖י', 'יִּשְׁלַ֤ח'), ('ךָ', 'אַבְרָהָם֙'), ('שֵֽׁב', 'אֶת'), ('וּ', 'יָדֹ֔ו'), ('לְ', 'וַ'), ('שָׂרָ֣ה', 'יִּקַּ֖ח'), ('אָמַ֗ר', 'אֶת'), ('הִנֵּ֨ה', 'הַֽ'), ('נָתַ֜תִּי', 'מַּאֲכֶ֑לֶת'), ('אֶ֤לֶף', 'לִ'), ('כֶּ֨סֶף֙', 'שְׁחֹ֖ט'), ('לְ', 'אֶת'), ('אָחִ֔י', 'בְּנֹֽו'), ('ךְ', 'וַ'), ('הִנֵּ֤ה', 'יִּקְרָ֨א'), ('הוּא', 'אֵלָ֜יו'), ('לָ', 'מַלְאַ֤ךְ'), ('ךְ֙', 'יְהוָה֙'), ('כְּס֣וּת', 'מִן'), ('עֵינַ֔יִם', 'הַ'), ('לְ', 'שָּׁמַ֔יִם'), ('כֹ֖ל', 'וַ'), ('אֲשֶׁ֣ר', 'יֹּ֖אמֶר'), ('אִתָּ֑', 'אַבְרָהָ֣ם'), ('ךְ', 'אַבְרָהָ֑ם'), ('וְ', 'וַ'), ('אֵ֥ת', 'יֹּ֖אמֶר'), ('כֹּ֖ל', 'הִנֵּֽנִי'), ('וְ', 'וַ'), ('נֹכָֽחַת', 'יֹּ֗אמֶר'), ('וַ', 'אַל'), ('יִּתְפַּלֵּ֥ל', 'תִּשְׁלַ֤ח'), ('אַבְרָהָ֖ם', 'יָֽדְךָ֙'), ('אֶל', 'אֶל'), ('הָ', 'הַ'), ('אֱלֹהִ֑ים', 'נַּ֔עַר'), ('וַ', 'וְ'), ('יִּרְפָּ֨א', 'אַל'), ('אֱלֹהִ֜ים', 'תַּ֥עַשׂ'), ('אֶת', 'לֹ֖ו'), ('אֲבִימֶ֧לֶךְ', 'מְא֑וּמָּה'), ('וְ', 'כִּ֣י'), ('אֶת', 'עַתָּ֣ה'), ('אִשְׁתּ֛', 'יָדַ֗עְתִּי'), ('וֹ', 'כִּֽי'), ('וְ', 'יְרֵ֤א'), ('אַמְהֹתָ֖י', 'אֱלֹהִים֙'), ('ו', 'אַ֔תָּה'), ('וַ', 'וְ'), ('יֵּלֵֽדוּ', 'לֹ֥א'), ('כִּֽי', 'חָשַׂ֛כְתָּ'), ('עָצֹ֤ר', 'אֶת'), ('עָצַר֙', 'בִּנְךָ֥'), ('יְהוָ֔ה', 'אֶת'), ('בְּעַ֥ד', 'יְחִידְךָ֖'), ('כָּל', 'מִמֶּֽנִּי'), ('רֶ֖חֶם', 'וַ'), ('לְ', 'יִּשָּׂ֨א'), ('בֵ֣ית', 'אַבְרָהָ֜ם'), ('אֲבִימֶ֑לֶךְ', 'אֶת'), ('עַל', 'עֵינָ֗יו'), ('דְּבַ֥ר', 'וַ'), ('שָׂרָ֖ה', 'יַּרְא֙'), ('אֵ֥שֶׁת', 'וְ'), ('אַבְרָהָֽם', 'הִנֵּה'), ('וַֽ', 'אַ֔יִל'), ('יהוָ֛ה', 'אַחַ֕ר'), ('פָּקַ֥ד', 'נֶאֱחַ֥ז'), ('אֶת', 'בַּ'), ('שָׂרָ֖ה', ''), ('כַּ', 'סְּבַ֖ךְ'), ('אֲשֶׁ֣ר', 'בְּ'), ('אָמָ֑ר', 'קַרְנָ֑יו'), ('וַ', 'וַ'), ('יַּ֧עַשׂ', 'יֵּ֤לֶךְ'), ('יְהוָ֛ה', 'אַבְרָהָם֙'), ('לְ', 'וַ'), ('שָׂרָ֖ה', 'יִּקַּ֣ח'), ('כַּ', 'אֶת'), ('אֲשֶׁ֥ר', 'הָ'), ('דִּבֵּֽר', 'אַ֔יִל'), ('וַ', 'וַ'), (None, 'יַּעֲלֵ֥הוּ'), (None, 'לְ'), ('תַּהַר֩', 'עֹלָ֖ה'), ('וַ', 'תַּ֥חַת'), ('תֵּ֨לֶד', 'בְּנֹֽו'), ('שָׂרָ֧ה', 'וַ'), ('לְ', 'יִּקְרָ֧א'), ('אַבְרָהָ֛ם', 'אַבְרָהָ֛ם'), ('בֵּ֖ן', 'שֵֽׁם'), ('לִ', 'הַ'), ('זְקֻנָ֑י', 'מָּקֹ֥ום'), ('ו', 'הַ'), ('לַ', 'ה֖וּא'), ('', 'יְהוָ֣ה'), ('מּוֹעֵ֕ד', 'יִרְאֶ֑ה'), ('אֲשֶׁר', 'אֲשֶׁר֙'), ('דִּבֶּ֥ר', 'יֵאָמֵ֣ר'), ('אֹת֖', 'הַ'), ('וֹ', 'יֹּ֔ום'), ('אֱלֹהִֽים', 'בְּ'), ('וַ', 'הַ֥ר'), ('יִּקְרָ֨א', 'יְהוָ֖ה'), ('אַבְרָהָ֜ם', 'יֵרָאֶֽה'), ('אֶֽת', 'וַ'), ('שֶׁם', 'יִּקְרָ֛א'), ('בְּנ֧', 'מַלְאַ֥ךְ'), ('וֹ', 'יְהוָ֖ה'), ('הַ', 'אֶל'), ('נּֽוֹלַד', 'אַבְרָהָ֑ם'), ('ל֛', 'שֵׁנִ֖ית'), ('וֹ', 'מִן'), ('אֲשֶׁר', 'הַ'), ('יָלְדָה', 'שָּׁמָֽיִם'), ('לּ֥', 'וַ'), ('וֹ', 'יֹּ֕אמֶר'), ('שָׂרָ֖ה', 'בִּ֥י'), ('יִצְחָֽק', 'נִשְׁבַּ֖עְתִּי'), ('וַ', 'נְאֻם'), ('יָּ֤מָל', 'יְהוָ֑ה'), ('אַבְרָהָם֙', 'כִּ֗י'), ('אֶת', 'יַ֚עַן'), ('יִצְחָ֣ק', 'אֲשֶׁ֤ר'), ('בְּנ֔', 'עָשִׂ֨יתָ֙'), ('וֹ', 'אֶת'), ('בֶּן', 'הַ'), ('שְׁמֹנַ֖ת', 'דָּבָ֣ר'), ('יָמִ֑ים', 'הַ'), ('כַּ', 'זֶּ֔ה'), ('אֲשֶׁ֛ר', 'וְ'), ('צִוָּ֥ה', 'לֹ֥א'), ('אֹת֖', 'חָשַׂ֖כְתָּ'), ('וֹ', 'אֶת'), ('אֱלֹהִֽים', 'בִּנְךָ֥'), ('וְ', 'אֶת'), ('אַבְרָהָ֖ם', 'יְחִידֶֽךָ'), ('בֶּן', 'כִּֽי'), ('מְאַ֣ת', 'בָרֵ֣ךְ'), ('שָׁנָ֑ה', 'אֲבָרֶכְךָ֗'), ('בְּ', 'וְ'), ('הִוָּ֣לֶד', 'הַרְבָּ֨ה'), ('ל֔', 'אַרְבֶּ֤ה'), ('וֹ', 'אֶֽת'), ('אֵ֖ת', 'זַרְעֲךָ֙'), ('יִצְחָ֥ק', 'כְּ'), ('בְּנֽ', 'כֹוכְבֵ֣י'), ('וֹ', 'הַ'), ('וַ', 'שָּׁמַ֔יִם'), ('תֹּ֣אמֶר', 'וְ'), ('שָׂרָ֔ה', 'כַ'), ('צְחֹ֕ק', ''), ('עָ֥שָׂה', 'חֹ֕ול'), ('לִ֖', 'אֲשֶׁ֖ר'), ('י', 'עַל'), ('אֱלֹהִ֑ים', 'שְׂפַ֣ת'), ('כָּל', 'הַ'), ('הַ', 'יָּ֑ם'), ('שֹּׁמֵ֖עַ', 'וְ'), ('יִֽצְחַק', 'יִרַ֣שׁ'), ('לִֽ', 'זַרְעֲךָ֔'), ('י', 'אֵ֖ת'), ('וַ', 'שַׁ֥עַר'), ('תֹּ֗אמֶר', 'אֹיְבָֽיו'), ('מִ֤י', 'וְ'), ('מִלֵּל֙', 'הִתְבָּרֲכ֣וּ'), ('לְ', 'בְ'), ('אַבְרָהָ֔ם', 'זַרְעֲךָ֔'), ('הֵינִ֥יקָה', 'כֹּ֖ל'), ('בָנִ֖ים', 'גֹּויֵ֣י'), ('שָׂרָ֑ה', 'הָ'), ('כִּֽי', 'אָ֑רֶץ'), ('יָלַ֥דְתִּי', 'עֵ֕קֶב'), ('בֵ֖ן', 'אֲשֶׁ֥ר'), ('לִ', 'שָׁמַ֖עְתָּ'), ('זְקֻנָֽי', 'בְּ'), ('ו', 'קֹלִֽי'), ('וַ', 'וַ'), (None, 'יָּ֤שָׁב'), ('יִּגְדַּ֥ל', 'אַבְרָהָם֙'), ('הַ', 'אֶל'), ('יֶּ֖לֶד', 'נְעָרָ֔יו'), ('וַ', 'וַ'), ('יִּגָּמַ֑ל', 'יָּקֻ֛מוּ'), ('וַ', 'וַ'), (None, 'יֵּלְכ֥וּ'), ('יַּ֤עַשׂ', 'יַחְדָּ֖ו'), ('אַבְרָהָם֙', 'אֶל'), ('מִשְׁתֶּ֣ה', 'בְּאֵ֣ר'), ('גָד֔וֹל', 'שָׁ֑בַע'), ('בְּ', 'וַ'), ('י֖וֹם', 'יֵּ֥שֶׁב'), ('הִגָּמֵ֥ל', 'אַבְרָהָ֖ם'), ('אֶת', 'בִּ'), ('יִצְחָֽק', 'בְאֵ֥ר'), ('וַ', 'שָֽׁבַע'), ('תֵּ֨רֶא', 'וַ'), ('שָׂרָ֜ה', 'יְהִ֗י'), ('אֶֽת', 'אַחֲרֵי֙'), ('בֶּן', 'הַ'), ('הָגָ֧ר', 'דְּבָרִ֣ים'), ('הַ', 'הָ'), ('מִּצְרִ֛ית', 'אֵ֔לֶּה'), ('אֲשֶׁר', 'וַ'), ('יָלְדָ֥ה', 'יֻּגַּ֥ד'), ('לְ', 'לְ'), ('אַבְרָהָ֖ם', 'אַבְרָהָ֖ם'), ('מְצַחֵֽק', 'לֵ'), ('וַ', 'אמֹ֑ר'), ('תֹּ֨אמֶר֙', 'הִ֠נֵּה'), ('לְ', 'יָלְדָ֨ה'), ('אַבְרָהָ֔ם', 'מִלְכָּ֥ה'), ('גָּרֵ֛שׁ', 'גַם'), ('הָ', 'הִ֛וא'), ('אָמָ֥ה', 'בָּנִ֖ים'), ('הַ', 'לְ'), ('זֹּ֖את', 'נָחֹ֥ור'), ('וְ', 'אָחִֽיךָ'), ('אֶת', 'אֶת'), ('בְּנָ֑', None), ('הּ', 'ע֥וּץ'), ('כִּ֣י', 'בְּכֹרֹ֖ו'), ('לֹ֤א', 'וְ'), ('יִירַשׁ֙', 'אֶת'), ('בֶּן', 'בּ֣וּז'), ('הָ', 'אָחִ֑יו'), ('אָמָ֣ה', 'וְ'), ('הַ', 'אֶת'), ('זֹּ֔את', 'קְמוּאֵ֖ל'), ('עִם', 'אֲבִ֥י'), ('בְּנִ֖', 'אֲרָֽם'), ('י', 'וְ'), ('עִם', 'אֶת'), ('יִצְחָֽק', 'כֶּ֣שֶׂד'), ('וַ', 'וְ'), ('יֵּ֧רַע', 'אֶת'), ('הַ', 'חֲזֹ֔ו'), ('דָּבָ֛ר', 'וְ'), ('מְאֹ֖ד', 'אֶת'), ('בְּ', 'פִּלְדָּ֖שׁ'), ('עֵינֵ֣י', 'וְ'), ('אַבְרָהָ֑ם', 'אֶת'), ('עַ֖ל', 'יִדְלָ֑ף'), ('אוֹדֹ֥ת', 'וְ'), ('בְּנֽ', 'אֵ֖ת'), ('וֹ', 'בְּתוּאֵֽל'), ('וַ', 'וּ'), ('יֹּ֨אמֶר', 'בְתוּאֵ֖ל'), ('אֱלֹהִ֜ים', 'יָלַ֣ד'), ('אֶל', 'אֶת'), ('אַבְרָהָ֗ם', 'רִבְקָ֑ה'), ('אַל', 'שְׁמֹנָ֥ה'), ('יֵרַ֤ע', 'אֵ֨לֶּה֙'), ('בְּ', 'יָלְדָ֣ה'), ('עֵינֶ֨י', 'מִלְכָּ֔ה'), ('ךָ֙', 'לְ'), ('עַל', 'נָחֹ֖ור'), ('הַ', 'אֲחִ֥י'), ('נַּ֣עַר', 'אַבְרָהָֽם'), ('וְ', 'וּ'), ('עַל', 'פִֽילַגְשֹׁ֖ו'), ('אֲמָתֶ֔', 'וּ'), ('ךָ', 'שְׁמָ֣הּ'), ('כֹּל֩', 'רְאוּמָ֑ה'), ('אֲשֶׁ֨ר', 'וַ'), ('תֹּאמַ֥ר', 'תֵּ֤לֶד'), ('אֵלֶ֛י', 'גַּם'), ('ךָ', 'הִוא֙'), ('שָׂרָ֖ה', 'אֶת'), ('שְׁמַ֣ע', 'טֶ֣בַח'), ('בְּ', 'וְ'), ('קֹלָ֑', 'אֶת'), ('הּ', 'גַּ֔חַם'), ('כִּ֣י', 'וְ'), ('בְ', 'אֶת'), ('יִצְחָ֔ק', 'תַּ֖חַשׁ'), ('יִקָּרֵ֥א', 'וְ'), ('לְ', 'אֶֽת'), ('ךָ֖', 'מַעֲכָֽה'), ('זָֽרַע', 'וַ'), ('וְ', 'יִּהְיוּ֙'), ('גַ֥ם', 'חַיֵּ֣י'), ('אֶת', 'שָׂרָ֔ה'), ('בֶּן', 'מֵאָ֥ה'), ('הָ', 'שָׁנָ֛ה'), ('אָמָ֖ה', 'וְ'), ('לְ', 'עֶשְׂרִ֥ים'), ('ג֣וֹי', 'שָׁנָ֖ה'), ('אֲשִׂימֶ֑', 'וְ'), ('נּוּ', 'שֶׁ֣בַע'), ('כִּ֥י', 'שָׁנִ֑ים'), ('זַרְעֲ', 'שְׁנֵ֖י'), ('ךָ֖', 'חַיֵּ֥י'), ('הֽוּא', 'שָׂרָֽה'), ('וַ', 'וַ'), ('יַּשְׁכֵּ֣ם', None), ('אַבְרָהָ֣ם', None), ('בַּ', 'תָּ֣מָת'), ('', 'שָׂרָ֗ה'), ('בֹּ֡קֶר', 'בְּ'), ('וַ', 'קִרְיַ֥ת אַרְבַּ֛ע'), ('יִּֽקַּֽח', 'הִ֥וא'), ('לֶחֶם֩', 'חֶבְרֹ֖ון'), ('וְ', 'בְּ'), ('חֵ֨מַת', 'אֶ֣רֶץ'), ('מַ֜יִם', 'כְּנָ֑עַן'), ('וַ', 'וַ'), ('יִּתֵּ֣ן', None), ('אֶל', 'יָּבֹא֙'), ('הָ֠גָר', 'אַבְרָהָ֔ם'), ('שָׂ֧ם', 'לִ'), ('עַל', 'סְפֹּ֥ד'), ('שִׁכְמָ֛', 'לְ'), ('הּ', 'שָׂרָ֖ה'), ('וְ', 'וְ'), ('אֶת', 'לִ'), ('הַ', 'בְכֹּתָֽהּ'), ('יֶּ֖לֶד', 'וַ'), ('וַֽ', 'יָּ֨קָם֙'), ('יְשַׁלְּחֶ֑', 'אַבְרָהָ֔ם'), ('הָ', 'מֵ'), ('וַ', 'עַ֖ל'), ('תֵּ֣לֶךְ', 'פְּנֵ֣י'), ('וַ', 'מֵתֹ֑ו'), ('תֵּ֔תַע', 'וַ'), ('בְּ', 'יְדַבֵּ֥ר'), ('מִדְבַּ֖ר', 'אֶל'), ('בְּאֵ֥ר', 'בְּנֵי'), ('שָֽׁבַע', 'חֵ֖ת'), ('וַ', 'לֵ'), ('יִּכְל֥וּ', 'אמֹֽר'), ('הַ', 'גֵּר'), ('מַּ֖יִם', 'וְ'), ('מִן', 'תֹושָׁ֥ב'), ('הַ', 'אָנֹכִ֖י'), ('חֵ֑מֶת', 'עִמָּכֶ֑ם'), ('וַ', 'תְּנ֨וּ'), ('תַּשְׁלֵ֣ךְ', 'לִ֤י'), ('אֶת', 'אֲחֻזַּת'), ('הַ', 'קֶ֨בֶר֙'), ('יֶּ֔לֶד', 'עִמָּכֶ֔ם'), ('תַּ֖חַת', 'וְ'), ('אַחַ֥ד', 'אֶקְבְּרָ֥ה'), ('הַ', 'מֵתִ֖י'), ('שִּׂיחִֽם', 'מִ'), ('וַ', 'לְּ'), ('תֵּלֶךְ֩', 'פָנָֽי'), ('וַ', 'וַ'), ('תֵּ֨שֶׁב', 'יַּעֲנ֧וּ'), ('לָ֜', 'בְנֵי'), ('הּ', 'חֵ֛ת'), ('מִ', 'אֶת'), ('נֶּ֗גֶד', 'אַבְרָהָ֖ם'), ('הַרְחֵק֙', 'לֵ'), ('כִּ', 'אמֹ֥ר'), ('מְטַחֲוֵ֣י', 'לֹֽו'), ('קֶ֔שֶׁת', 'שְׁמָעֵ֣נוּ'), ('כִּ֣י', 'אֲדֹנִ֗י'), ('אָֽמְרָ֔ה', 'נְשִׂ֨יא'), ('אַל', 'אֱלֹהִ֤ים'), ('אֶרְאֶ֖ה', 'אַתָּה֙'), ('בְּ', 'בְּ'), ('מ֣וֹת', 'תֹוכֵ֔נוּ'), ('הַ', 'בְּ'), ('יָּ֑לֶד', 'מִבְחַ֣ר'), ('וַ', 'קְבָרֵ֔ינוּ'), ('תֵּ֣שֶׁב', 'קְבֹ֖ר'), ('מִ', 'אֶת'), ('נֶּ֔גֶד', 'מֵתֶ֑ךָ'), ('וַ', 'אִ֣ישׁ'), ('תִּשָּׂ֥א', 'מִמֶּ֔נּוּ'), ('אֶת', 'אֶת'), ('קֹלָ֖', None), ('הּ', 'קִבְרֹ֛ו'), ('וַ', 'לֹֽא'), ('תֵּֽבְךְּ', 'יִכְלֶ֥ה'), ('וַ', 'מִמְּךָ֖'), ('יִּשְׁמַ֣ע', 'מִ'), ('אֱלֹהִים֮', 'קְּבֹ֥ר'), ('אֶת', 'מֵתֶֽךָ'), ('ק֣וֹל', 'וַ'), ('הַ', 'יָּ֧קָם'), ('נַּעַר֒', 'אַבְרָהָ֛ם'), ('וַ', 'וַ'), ('יִּקְרָא֩', 'יִּשְׁתַּ֥חוּ'), ('מַלְאַ֨ךְ', 'לְ'), ('אֱלֹהִ֤ים', 'עַם'), ('אֶל', 'הָ'), ('הָגָר֙', 'אָ֖רֶץ'), ('מִן', 'לִ'), ('הַ', 'בְנֵי'), ('שָּׁמַ֔יִם', 'חֵֽת'), ('וַ', 'וַ'), ('יֹּ֥אמֶר', 'יְדַבֵּ֥ר'), ('לָ֖', 'אִתָּ֖ם'), ('הּ', 'לֵ'), ('מַה', 'אמֹ֑ר'), ('לָּ֣', 'אִם'), ('ךְ', 'יֵ֣שׁ'), ('הָגָ֑ר', 'אֶֽת'), ('אַל', 'נַפְשְׁכֶ֗ם'), ('תִּ֣ירְאִ֔י', 'לִ'), ('כִּֽי', 'קְבֹּ֤ר'), ('שָׁמַ֧ע', 'אֶת'), ('אֱלֹהִ֛ים', 'מֵתִי֙'), ('אֶל', 'מִ'), ('ק֥וֹל', 'לְּ'), ('הַ', 'פָנַ֔י'), ('נַּ֖עַר', 'שְׁמָע֕וּנִי'), ('בַּ', 'וּ'), ('אֲשֶׁ֥ר', 'פִגְעוּ'), ('הוּא', 'לִ֖י'), ('שָֽׁם', 'בְּ'), ('ק֚וּמִי', 'עֶפְרֹ֥ון'), ('שְׂאִ֣י', 'בֶּן'), ('אֶת', 'צֹֽחַר'), ('הַ', 'וְ'), ('נַּ֔עַר', 'יִתֶּן'), ('וְ', 'לִ֗י'), ('הַחֲזִ֥יקִי', 'אֶת'), ('אֶת', 'מְעָרַ֤ת'), ('יָדֵ֖', 'הַ'), ('ךְ', 'מַּכְפֵּלָה֙'), ('בּ֑', 'אֲשֶׁר'), ('וֹ', 'לֹ֔ו'), ('כִּֽי', 'אֲשֶׁ֖ר'), ('לְ', 'בִּ'), ('ג֥וֹי', 'קְצֵ֣ה'), ('גָּד֖וֹל', 'שָׂדֵ֑הוּ'), ('אֲשִׂימֶֽ', 'בְּ'), ('נּוּ', 'כֶ֨סֶף'), ('וַ', 'מָלֵ֜א'), ('יִּפְקַ֤ח', 'יִתְּנֶ֥נָּה'), ('אֱלֹהִים֙', 'לִ֛י'), ('אֶת', 'בְּ'), ('עֵינֶ֔י', 'תֹוכְכֶ֖ם'), ('הָ', 'לַ'), ('וַ', 'אֲחֻזַּת'), ('תֵּ֖רֶא', 'קָֽבֶר'), ('בְּאֵ֣ר', 'וְ'), ('מָ֑יִם', 'עֶפְרֹ֥ון'), ('וַ', 'יֹשֵׁ֖ב'), ('תֵּ֜לֶךְ', 'בְּ'), ('וַ', 'תֹ֣וךְ'), ('תְּמַלֵּ֤א', 'בְּנֵי'), ('אֶת', 'חֵ֑ת'), ('הַ', 'וַ'), ('חֵ֨מֶת֙', 'יַּעַן֩'), ('מַ֔יִם', 'עֶפְרֹ֨ון'), ('וַ', 'הַ'), ('תַּ֖שְׁקְ', 'חִתִּ֤י'), ('אֶת', 'אֶת'), ('הַ', 'אַבְרָהָם֙'), ('נָּֽעַר', 'בְּ'), ('וַ', 'אָזְנֵ֣י'), ('יְהִ֧י', 'בְנֵי'), ('אֱלֹהִ֛ים', 'חֵ֔ת'), ('אֶת', 'לְ'), ('הַ', 'כֹ֛ל'), ('נַּ֖עַר', 'בָּאֵ֥י'), ('וַ', 'שַֽׁעַר'), ('יִּגְדָּ֑ל', 'עִירֹ֖ו'), ('וַ', 'לֵ'), ('יֵּ֨שֶׁב֙', 'אמֹֽר'), ('בַּ', 'לֹֽא'), ('', 'אֲדֹנִ֣י'), ('מִּדְבָּ֔ר', 'שְׁמָעֵ֔נִי'), ('וַ', 'הַ'), ('יְהִ֖י', 'שָּׂדֶה֙'), ('רֹבֶ֥ה', 'נָתַ֣תִּי'), ('קַשָּֽׁת', 'לָ֔ךְ'), ('וַ', 'וְ'), ('יֵּ֖שֶׁב', 'הַ'), ('בְּ', 'מְּעָרָ֥ה'), ('מִדְבַּ֣ר', 'אֲשֶׁר'), ('פָּארָ֑ן', 'בֹּ֖ו'), ('וַ', 'לְךָ֣'), ('תִּֽקַּֽח', 'נְתַתִּ֑יהָ'), ('ל֥', 'לְ'), ('וֹ', 'עֵינֵ֧י'), ('אִמּ֛', 'בְנֵי'), ('וֹ', 'עַמִּ֛י'), ('אִשָּׁ֖ה', 'נְתַתִּ֥יהָ'), ('מֵ', 'לָּ֖ךְ'), ('אֶ֥רֶץ', 'קְבֹ֥ר'), ('מִצְרָֽיִם', 'מֵתֶֽךָ'), ('וַֽ', 'וַ'), ('יְהִי֙', 'יִּשְׁתַּ֨חוּ֙'), ('בָּ', 'אַבְרָהָ֔ם'), ('', 'לִ'), ('עֵ֣ת', 'פְנֵ֖י'), ('הַ', 'עַ֥ם'), ('הִ֔וא', 'הָ'), ('וַ', 'אָֽרֶץ'), ('יֹּ֣אמֶר', 'וַ'), ('אֲבִימֶ֗לֶךְ', 'יְדַבֵּ֨ר'), ('וּ', 'אֶל'), ('פִיכֹל֙', 'עֶפְרֹ֜ון'), ('שַׂר', 'בְּ'), ('צְבָא֔', 'אָזְנֵ֤י'), ('וֹ', 'עַם'), ('אֶל', 'הָ'), ('אַבְרָהָ֖ם', 'אָ֨רֶץ֙'), ('לֵ', 'לֵ'), ('אמֹ֑ר', None), ('אֱלֹהִ֣ים', 'אמֹ֔ר'), ('עִמְּ', 'אַ֛ךְ'), ('ךָ֔', 'אִם'), ('בְּ', 'אַתָּ֥ה'), ('כֹ֥ל', 'ל֖וּ'), ('אֲשֶׁר', 'שְׁמָעֵ֑נִי'), ('אַתָּ֖ה', 'נָתַ֜תִּי'), ('עֹשֶֽׂה', 'כֶּ֤סֶף'), ('וְ', 'הַ'), ('עַתָּ֗ה', 'שָּׂדֶה֙'), ('הִשָּׁ֨בְעָ', 'קַ֣ח'), ('ה', 'מִמֶּ֔נִּי'), ('לִּ֤', 'וְ'), ('י', 'אֶקְבְּרָ֥ה'), ('בֵֽ', 'אֶת'), ('אלֹהִים֙', 'מֵתִ֖י'), ('הֵ֔נָּה', 'שָֽׁמָּה'), ('אִם', 'וַ'), ('תִּשְׁקֹ֣ר', 'יַּ֧עַן'), ('לִ֔', 'עֶפְרֹ֛ון'), ('י', 'אֶת'), ('וּ', 'אַבְרָהָ֖ם'), ('לְ', 'לֵ'), ('נִינִ֖', 'אמֹ֥ר'), ('י', 'לֹֽו'), ('וּ', 'אֲדֹנִ֣י'), ('לְ', 'שְׁמָעֵ֔נִי'), ('נֶכְדִּ֑', 'אֶרֶץ֩'), ('י', 'אַרְבַּ֨ע'), ('כַּ', 'מֵאֹ֧ת'), ('', 'שֶֽׁקֶל'), ('חֶ֜סֶד', 'כֶּ֛סֶף'), ('אֲשֶׁר', 'בֵּינִ֥י'), ('עָשִׂ֤יתִי', 'וּ'), ('עִמְּ', 'בֵֽינְךָ֖'), ('ךָ֙', 'מַה'), ('תַּעֲשֶׂ֣ה', 'הִ֑וא'), ('עִמָּדִ֔', 'וְ'), ('י', 'אֶת'), ('וְ', 'מֵתְךָ֖'), ('עִם', 'קְבֹֽר'), ('הָ', 'וַ'), ('אָ֖רֶץ', 'יִּשְׁמַ֣ע'), ('אֲשֶׁר', 'אַבְרָהָם֮'), ('גַּ֥רְתָּה', 'אֶל')]\n"
     ]
    }
   ],
   "source": [
    "print(aligned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
