{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">TF-app:</b> <span title=\"rv2.4.0=#571428c995d309c43ff592f885a5ebb42a87be6f offline under ~/text-fabric-data\">~/text-fabric-data/annotation/app-bhsa/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv1.7.3=#af70cebe9d23ea736bb40e3ccd6768875b52a49d offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/bhsa/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv2.1=#aba4367b49750089e4e4122415a77cac43bd97bc offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/phono/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv2.1=#f45f6cc3c4f933dba6e649f49cdb14a40dcf333f offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/parallels/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 9.1.1\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "120 features found and 0 ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 9.1.1</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa TF-app\">app-bhsa v3</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/0_home\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Parallel Passages</b></summary><b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"~/text-fabric-data/etcbc/parallels/tf/c/crossref.tf\">crossref</a></i></b><br></details><details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book.tf\">book</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book@am.tf\">book@ll</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/code.tf\">code</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/det.tf\">det</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/domain.tf\">domain</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/function.tf\">function</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons.tf\">g_cons</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons_utf8.tf\">g_cons_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex.tf\">g_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex_utf8.tf\">g_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word.tf\">g_word</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word_utf8.tf\">g_word_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/label.tf\">label</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/language.tf\">language</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex.tf\">lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex_utf8.tf\">lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ls.tf\">ls</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nametype.tf\">nametype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nme.tf\">nme</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/number.tf\">number</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pargr.tf\">pargr</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pdp.tf\">pdp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pfm.tf\">pfm</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs.tf\">prs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_gn.tf\">prs_gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_nu.tf\">prs_nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_ps.tf\">prs_ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ps.tf\">ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer.tf\">qere_trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer_utf8.tf\">qere_trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_utf8.tf\">qere_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rank_lex.tf\">rank_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rela.tf\">rela</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/sp.tf\">sp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/st.tf\">st</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/tab.tf\">tab</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer.tf\">trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer_utf8.tf\">trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/txt.tf\">txt</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/typ.tf\">typ</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/uvf.tf\">uvf</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbe.tf\">vbe</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbs.tf\">vbs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex.tf\">voc_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex_utf8.tf\">voc_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vs.tf\">vs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vt.tf\">vt</a><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/mother.tf\">mother</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/oslots.tf\">oslots</a></i></b><br></details><details><summary><b>Phonetic Transcriptions</b></summary><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono.tf\">phono</a><br><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono_trailer.tf\">phono_trailer</a><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".ubd {\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".col {\n",
       "   display: inline-block;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 2px;\n",
       "    margin: 2px;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -2px 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 2px 0px;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".tfsechead {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--tfsechead);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  10px 2px 2px 2px;\n",
       "    margin: 16px 2px 2px 2px;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 4px;\n",
       "    margin-left: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 4px;\n",
       "    margin-right: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -24px;\n",
       "    margin-left: 20px;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 6px;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 2px;\n",
       "    margin-left: 2px;\n",
       "    padding: 2px 2px;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 40px;\n",
       "\tpadding: 20px;\n",
       "\tborder: 2px solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--tfsechead:          hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         3px;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   2px;\n",
       "  --border-width0:      2px;\n",
       "  --border-width1:      3px;\n",
       "  --border-width2:      4px;\n",
       "  --border-width3:      6px;\n",
       "  --border-width4:      5px;\n",
       "  --border-width-plain: 2px;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 2px;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 4px ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 4px ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 6px;\n",
       "  padding: 4px;\n",
       "  margin: 4px;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>Text-Fabric API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"doc\">N F E L T S C TF</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tf.app import use\n",
    "import math\n",
    "A = use('bhsa', hoist=globals(), checkout='local', version='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class with data used to assign difficulty weights to passages based\n",
    "on the lexical frequencies of words in the passage. \n",
    "\n",
    "ranks: a list of string categories for lexical frequency ranges\n",
    "ranges: a 2D list of the numeric range for each rank\n",
    "weights: a list of the weight penalties assigned per word for each rank\n",
    "\"\"\"\n",
    "class Rank:\n",
    "\n",
    "    def __init__(self, name, ranks, ranges, weights):\n",
    "        self.name = name\n",
    "        self.ranks = ranks \n",
    "        self.ranges = ranges \n",
    "        self.weights = weights \n",
    "\n",
    "    # Auxiliary function to create a single rank_scale dictionary.\n",
    "    def get_rank_dict(self):\n",
    "        rank_dict = {}\n",
    "        for i in range(len(self.ranks)):\n",
    "            rank_dict[self.ranks[i]] = {\n",
    "                'range': self.ranges[i],\n",
    "                'weight': self.weights[i]\n",
    "            }\n",
    "        return rank_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "A class to store different ranking scales. \n",
    "\"\"\"\n",
    "class LexRanks:\n",
    "\n",
    "    # Using 2-elem lists is far faster than searching entire ranges. \n",
    "    # Rather than if i in range(), check if i > l[0] and <= l[1].\n",
    "    # Using this method scales runtime from ~0:04:30 to ~0:00:15.\n",
    "    _3_ranks = Rank(\n",
    "        \"3_ranks\",\n",
    "        ['Frequent', 'Uncommon', 'Rare'],\n",
    "        [\n",
    "            [100, 51000],\n",
    "            [10, 100],\n",
    "            [1, 10],\n",
    "        ],\n",
    "        [1, 3, 7]\n",
    "    )\n",
    "   \n",
    "    _4_ranks = Rank(\n",
    "        \"4_ranks\",\n",
    "        ['Frequent', 'Medium', 'Uncommon', 'Rare'],\n",
    "        [\n",
    "            [100, 51000],\n",
    "            [50, 100],\n",
    "            [10, 50],\n",
    "            [1, 10],\n",
    "        ],\n",
    "        [1, 4, 5, 8]\n",
    "    )\n",
    "\n",
    "    _5_ranks_a = Rank(\n",
    "        \"5a_ranks\",\n",
    "        ['Frequent', 'Common', 'Medium', 'Uncommon', 'Rare'],\n",
    "        [\n",
    "            [500, 51000],\n",
    "            [250, 500],\n",
    "            [150, 250],\n",
    "            [50, 150],\n",
    "            [1, 50],\n",
    "        ],\n",
    "        [1, 2, 3, 5, 8]\n",
    "    )\n",
    "\n",
    "    _5_ranks_b = Rank(\n",
    "        \"5b_ranks\",\n",
    "        ['Frequent', 'Common', 'Infrequent', 'Rare', 'Scarce'],\n",
    "        [\n",
    "            [200, 51000],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [20, 50],\n",
    "            [1, 20],\n",
    "        ],\n",
    "        [1, 1.5, 3, 5, 8]\n",
    "    )\n",
    "\n",
    "    _7_ranks = Rank(\n",
    "        \"7_ranks\",\n",
    "        ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Scarce'],\n",
    "        [\n",
    "            [800, 51000],\n",
    "            [400, 800],\n",
    "            [200, 400],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [15, 50],\n",
    "            [1, 15],\n",
    "        ],\n",
    "        [1, 1.1, 1.3, 1.7, 3, 5.5, 8.5]\n",
    "    )\n",
    "\n",
    "    _9_ranks = Rank(\n",
    "        \"9_ranks\",\n",
    "        ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Scarce', 'Scarcer', 'Scarcest'],\n",
    "        [\n",
    "            [1000, 51000],\n",
    "            [400, 1000],\n",
    "            [200, 400],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [30, 50],\n",
    "            [20, 30],\n",
    "            [10, 20],\n",
    "            [1, 10]\n",
    "        ],\n",
    "        [1, 1.1, 1.3, 1.7, 3, 5.5, 8, 9, 10]\n",
    "    )\n",
    "\n",
    "    _10_ranks = Rank(\n",
    "        \"10_ranks\",\n",
    "        ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Rarer', 'Scarce', 'Scarcer', 'Scarcest'],\n",
    "        [\n",
    "            [1000, 51000],\n",
    "            [400, 1000],\n",
    "            [200, 400],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [40, 50],\n",
    "            [30, 40],\n",
    "            [20, 30],\n",
    "            [10, 20],\n",
    "            [1, 10]\n",
    "        ],\n",
    "        [1, 1.1, 1.3, 1.7, 3, 5.5, 7, 8, 9, 10]\n",
    "    )\n",
    "\n",
    "    all_ranks = [\n",
    "        _3_ranks,\n",
    "        _4_ranks,\n",
    "        _5_ranks_a,\n",
    "        _5_ranks_b,\n",
    "        _7_ranks,\n",
    "        _9_ranks,\n",
    "        _10_ranks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include morphology penalties.\n",
    "class MorphRank:\n",
    "    other = 8\n",
    "    base = 0\n",
    "\n",
    "    stem_map = {\n",
    "        'hif':2,\t#hif‘il\n",
    "        'hit':3,\t#hitpa“el\n",
    "        'htpo':other,\t#hitpo“el\n",
    "        'hof':5,\t#hof‘al\n",
    "        'nif':3,\t#nif‘al\n",
    "        'piel':2,\t#pi“el\n",
    "        'poal':other,\t#po“al\n",
    "        'poel':other,\t#po“el\n",
    "        'pual':5,\t#pu“al\n",
    "        'qal':base\t#qal\n",
    "    }\n",
    "    tense_map = {\n",
    "        'perf':base,\t#perfect\n",
    "        'impf':2,\t#imperfect\n",
    "        'wayq':base,\t#wayyiqtol\n",
    "        'impv':3.5,\t#imperative\n",
    "        'infa':5,\t#infinitive (absolute)\n",
    "        'infc':2,\t#infinitive (construct)\n",
    "        'ptca':3,\t#participle\n",
    "        'ptcp':5,\t#participle (passive)\n",
    "    }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class that contains data to help assign difficulty weights to \n",
    "any portion of Hebrew text that has more than one word. \n",
    "\"\"\"\n",
    "class Classify:\n",
    "    \"\"\" \n",
    "    Notes on stop_words_types and other exclusion lists. \n",
    "\n",
    "    Most prepositions, articles, and conjunctions don't\n",
    "    add any meaningul weight to a text and could thus be exlcuded.\n",
    "    \n",
    "    Example use:\n",
    "    words = [w for w in passage if F.sp.v(w) not in stop_words_types]\n",
    "    \n",
    "    Note: the only Heb article is 'הַ' with 30,386 occurences. There are some \n",
    "    preps and conjs that have few occurences, so I recommend not using\n",
    "    stop_words_types when weighing passages and using stop_words instead.\n",
    "    \"\"\"\n",
    "    stop_words_types = ['prep', 'art', 'conj']\n",
    "    # Check if F.voc_lex_utf8.v(word) is in this list. If\n",
    "    # so it can be excluded since it occurs so often. \n",
    "    stop_words = ['אֵת', 'בְּ', 'לְ', 'הַ', 'וְ']\n",
    "    # If you take verb data into account when weighing a\n",
    "    # paragraph, these common types could be excluded. \n",
    "    easy_vtypes = ['perf', 'impf', 'wayq']\n",
    "    easy_vstems = ['qal', 'hif', 'nif', 'piel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hebrew Passage Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class that contains a Hebrew passage, consisting of paragraphs \n",
    "as marked by a petach (פ) or samech (ס) in the Masoretic Text. If \n",
    "a book like Psalms, which lacks paragaph markers, is encountered,\n",
    "the passages are split at the chapter level. \n",
    "\"\"\"\n",
    "class Passage:\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.verses = [] # a list of verse node ints. \n",
    "        self.words = [] # a list of word node ints. \n",
    "        self.start_word = 0\n",
    "        self.end_word = 0\n",
    "        self.start_verse = 0\n",
    "        self.end_verse = 0\n",
    "        self.word_count = 0\n",
    "        self.weight0 = 0\n",
    "        self.weight1 = 0\n",
    "        self.weight2a = 0 # all words denom\n",
    "        self.weight2b = 0 # unique words\n",
    "        self.weight3a = 0 # all word denom\n",
    "        self.weight3b = 0\n",
    "        self.weight3c = 0 #3a with morph\n",
    "        self.verb_types_present = set()\n",
    "        self.verb_stems_present = set()\n",
    "        self.word_ranks_data = {}\n",
    "        self.start_ref = ''\n",
    "        self.end_ref = ''\n",
    "\n",
    "    paragraph_markers = {'פ': 'open', 'ס': 'closed'}\n",
    "\n",
    "    # Returns a list of all words present in the passage.\n",
    "    def get_all_words(self):\n",
    "        words = []\n",
    "        for verse in self.verses:\n",
    "            for word in L.i(verse, otype='word'):\n",
    "                words.append(word)\n",
    "        return words\n",
    "\n",
    "    # Returns a list of all words present in a specified verse in the passage.\n",
    "    def get_vs_words(self, verse):\n",
    "        verse_words = [w for w in L.i(verse, otype='word')]\n",
    "        return verse_words\n",
    "\n",
    "    # Returns a String of all the text in the passage.\n",
    "    def get_text(self):\n",
    "        return T.text(self.verses, fmt='text-orig-full')\n",
    "\n",
    "    \"\"\"\n",
    "    get_vs_weights() returns a dictionary mapping each verse node in \n",
    "    the passage to a weight. It takes rank_scale as input, an instance\n",
    "    of Classify(args).rank_scale() (see notes in Classify for instantiaion).\n",
    "    \"\"\"\n",
    "    def get_vs_weights(self, rank_scale):\n",
    "        # A dictionary mapping verse nodes to weights.\n",
    "        verse_weights = {}\n",
    "        # Iterate over verses in the passage.\n",
    "        for verse in self.verses:\n",
    "            verse_weight = 0\n",
    "            words = self.get_vs_words(verse)\n",
    "            # Add the scaled word weights to the verse's total weight.\n",
    "            for word in words:\n",
    "                if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                    for rank in rank_scale.keys():\n",
    "                        lex_freq = F.freq_lex.v(word)\n",
    "                        _range = rank_scale[rank]['range']\n",
    "                        if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                            verse_weight += rank_scale[rank]['weight']\n",
    "            # Add the verse's weight to the dictionary at this verse's key. \n",
    "            verse_weight /= len(words)\n",
    "            verse_weights[verse] = round(verse_weight, 4)\n",
    "        \n",
    "        return verse_weights\n",
    "\n",
    "    # Simply add the freq_lex of each word to weight.\n",
    "    def get_passage_weight0(self):\n",
    "        total_weight = 0\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                # Subtract 10000 to penalize rare words. \n",
    "                total_weight += 10000 - F.freq_lex.v(word)\n",
    "        total_weight /= len(self.words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    def get_passage_weight1(self, rank_scale):\n",
    "        total_weight = 0\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    _range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                            total_weight += (rank_scale[rank]['weight']) / 2\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            total_weight += rank_scale[rank]['weight']\n",
    "        total_weight /= len(self.words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    # Only penalize once per lexical value.  \n",
    "    def get_passage_weight2(self, rank_scale, div_all=True):\n",
    "        total_weight = 0\n",
    "        unique_words = set()\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            lex = F.voc_lex_utf8.v(word)\n",
    "            if lex not in Classify().stop_words and lex not in unique_words:\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    _range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                            total_weight += (rank_scale[rank]['weight']) / 2\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            total_weight += rank_scale[rank]['weight']\n",
    "                unique_words.add(lex)\n",
    "        # Compare using all words as denominator vs. unique words.\n",
    "        if div_all:\n",
    "            total_weight /= len(self.words)\n",
    "        else:\n",
    "            total_weight /= len(unique_words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    # Decrease penalty for each occurance. \n",
    "    def get_passage_weight3(self, rank_scale, div_all=True, morph=False):\n",
    "        word_weights = {}\n",
    "        verb_count = 0\n",
    "        verb_weight = 0\n",
    "        min_penalty = 1.7 # min penalty for rare words and proper nouns. \n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            lex = F.voc_lex_utf8.v(word)\n",
    "            if lex not in Classify().stop_words:\n",
    "                # Add partial penalty for reocurring words. \n",
    "                if lex in word_weights.keys():\n",
    "                    # Only gradually decrease penalty for rarer words. \n",
    "                    # Decreases by 1 point per occurance. \n",
    "                    word_weights[lex]['count'] += 1\n",
    "                    if F.freq_lex.v(word) < 100:\n",
    "                        count = word_weights[lex]['count']\n",
    "                        penalty = word_weights[lex]['penalty']\n",
    "                        new_weight = penalty - count \n",
    "                        added_weight = new_weight if new_weight >= min_penalty else min_penalty\n",
    "                        word_weights[lex]['weight'] += added_weight\n",
    "                    else:\n",
    "                        word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                # Add full penalty for the first occurance. \n",
    "                else:\n",
    "                    # Add word to hash table\n",
    "                    word_weights[lex] = {'count':0, 'weight':0, 'penalty':0}\n",
    "                    # Iterate over the ranks present in the rank scale. \n",
    "                    for rank in rank_scale.keys():\n",
    "                        lex_freq = F.freq_lex.v(word)\n",
    "                        _range = rank_scale[rank]['range']\n",
    "                        if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                            # Give a half penalty for proper nouns. \n",
    "                            _penalty = rank_scale[rank]['weight']\n",
    "                            if F.sp.v(word) == 'nmpr' and _penalty > min_penalty: # proper noun\n",
    "                                word_weights[lex]['penalty'] = int(math.ceil(_penalty / 2))\n",
    "                            # Give a full penalty for other word types. \n",
    "                            else:\n",
    "                                word_weights[lex]['penalty'] = _penalty\n",
    "                    word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                    word_weights[lex]['count'] += 1\n",
    "                # If we're penalizing for morphology\n",
    "                if morph:\n",
    "                    if F.sp.v(word) == 'verb':\n",
    "                        verb_count += 1\n",
    "                        verb_weight += MorphRank.stem_map.get(F.vs.v(word),0) + MorphRank.tense_map[F.vt.v(word)]\n",
    "\n",
    "        # Get the sum of all word weights. \n",
    "        total_weight = sum([w for w in [word_weights[k]['weight'] for k in word_weights.keys()]])\n",
    "        # Compare using all words as denominator vs. unique words.\n",
    "        if div_all:\n",
    "            total_weight = total_weight / len(self.words) + (verb_weight / len(self.words))\n",
    "        else:\n",
    "            total_weight /= len(word_weights)\n",
    "        \n",
    "        return round(total_weight, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(3//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class to store passages. It includes methods to sort the passages\n",
    "by attributes such as word count, weight, and canonical order. It\n",
    "also stores the rank scale used to create the passage list. The passages\n",
    "will be stored in order_sorted by default. \n",
    "\"\"\"\n",
    "class Passages:\n",
    "    \n",
    "    def __init__(self, passages, rank_scale={}):\n",
    "        self.rank_scale = rank_scale\n",
    "        self.order_sorted = passages\n",
    "        self.word_count_sorted = self.word_count_sort()\n",
    "        self.weight_sorted0 = self.weight_sort0()\n",
    "        self.weight_sorted1 = self.weight_sort1()\n",
    "        self.weight_sorted2a = self.weight_sort2a()\n",
    "        self.weight_sorted2b = self.weight_sort2b()\n",
    "        self.weight_sorted3a = self.weight_sort3a()\n",
    "        self.weight_sorted3b = self.weight_sort3b()\n",
    "        self.weight_sorted3c = self.weight_sort3c()\n",
    "    \n",
    "    def word_count_sort(self):\n",
    "        return sorted(self.order_sorted, key=lambda p: p.word_count)\n",
    "\n",
    "    \"\"\" For weight sorts I use a dict mapping Passage objects to their rank\n",
    "    number so that I can then compare all the weight sorts in a dataframe. \"\"\"\n",
    "    def weight_sort0(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight0)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort1(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight1)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort2a(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight2a)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort2b(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight2b)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort3a(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3a)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort3b(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3b)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    # Morph\n",
    "    def weight_sort3c(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3c)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    # A function to display the rank scale as a multi-line string. \n",
    "    def print_scale(self):\n",
    "        scale = self.rank_scale\n",
    "        output_text = \"\"\n",
    "        for i, rank in enumerate(scale.ranks):\n",
    "            _range = scale.ranges[i]\n",
    "            weight = scale.weights[i]\n",
    "            output = f\"{weight}\\t{_range[0]}-{_range[1]} occ\"\n",
    "            output_text += f\"{rank}:   \\t{output}\\n\"\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class used to compare mismatches between differently sorted lists. \n",
    "class CompareData:\n",
    "\n",
    "    def update_index(self, dict, key, i):\n",
    "        if key not in dict:\n",
    "            dict[key] = i\n",
    "        else:\n",
    "            dict[key] = i - dict[key]\n",
    "\n",
    "    def compare_mismatches(self, list_a, list_b, sorted=False):\n",
    "        mism = {} \n",
    "        for i, (a, b) in enumerate(zip(list_a, list_b)):\n",
    "            a, b = a.start_ref, b.start_ref\n",
    "            self.update_index(mism, a, i)\n",
    "            self.update_index(mism, b, i)\n",
    "        if sorted:\n",
    "            return {k: v for k, v in sorted(mism.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return mism\n",
    "\n",
    "    def average_mismatch(self, mism):\n",
    "        return sum(list(mism.values()))/len(mism)\n",
    "\n",
    "    def max_mismatch(self, mism):\n",
    "        return sorted(mism.items(), key=lambda item: item[1])[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passage Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used by get_passages()\n",
    "\n",
    "Check whether we have reached the end of a valid passage as defined by \n",
    "passage_size and paragraph markers. If we have, or if a new book (or new \n",
    "chapter in certain books like Psalms), then mark the passage as valid by\n",
    "setting its value to True. \n",
    "In certain cases we will not want to add the current verse to the current\n",
    "passage, so we will set add_verse to False. \n",
    "\"\"\"\n",
    "def valid_passage(passage, verse, passage_size_min, passage_size_max):\n",
    "    is_valid = False\n",
    "    add_verse = True\n",
    "    # Get the string value at the end of the verse. \n",
    "    verse_ending = T.text(verse).split()[-1]\n",
    "    verse_book = L.u(verse, otype='book')[0]\n",
    "    verse_chapter = L.u(verse, otype='chapter')[0]\n",
    "    verse_word_count = len(passage.get_vs_words(verse))\n",
    "    ps_119 = 427315 # node for Psalm 119.\n",
    "    # Check if we've reached a new book, if yes, end the paragraph.\n",
    "    if L.u(passage.verses[-1], otype='book')[0] != verse_book:\n",
    "        is_valid = True \n",
    "        add_verse = False\n",
    "    # Check if the current verse is in the following books.\n",
    "    # Since they lack enough paragraph markers to make meaningful passages,\n",
    "    # we create passages at the chapter level. \n",
    "    elif verse_book in [T.bookNode('Ruth'), T.bookNode('Jonah'), T.bookNode('Ecclesiastes'), T.bookNode('Psalms')]:\n",
    "        if verse_chapter != L.u(passage.verses[-1], otype='chapter')[0]:\n",
    "            is_valid = True \n",
    "            add_verse = False\n",
    "        # If Psalm 119, split up into 8 verse sections to preserve acrostic.\n",
    "        elif verse_chapter == ps_119:\n",
    "            if (verse-1) % 8 == 0:\n",
    "                is_valid = True \n",
    "                add_verse = False\n",
    "    # Otherwise check if we have reached the end of a paragraph. \n",
    "    elif verse_ending in passage.paragraph_markers.keys() \\\n",
    "    and len(passage.get_all_words()) + verse_word_count >= passage_size_min:\n",
    "        is_valid = True\n",
    "\n",
    "    # TODO Optimize this to create meaningful passages.\n",
    "    # Or if the passage is too long.\n",
    "    # ** the len(getAllWords) greatly increases the run time -- we need a way to optimize. \n",
    "    # elif len(passage.get_all_words()) + verse_word_count > passage_size_max:\n",
    "    #     is_valid = True \n",
    "    #     add_verse = False\n",
    "\n",
    "    return is_valid, add_verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used by get_passages\n",
    "\n",
    "Update all of the data of a passage instance once its end verse \n",
    "has been reached. \n",
    "\"\"\"\n",
    "def update_passage_data(passage, rank_scale):\n",
    "    # TODO to print nouns in red. \n",
    "    passage.word_ranks_data = {k:{'occ':0, 'words':set()} for k in rank_scale.keys()}\n",
    "    passage.words = passage.get_all_words()\n",
    "\n",
    "    passage.start_verse = passage.verses[0]\n",
    "    passage.end_verse = passage.verses[-1]\n",
    "    passage.start_word = passage.words[0]\n",
    "    passage.end_word = passage.words[-1]\n",
    "    passage.word_count = len(passage.words)\n",
    "\n",
    "    passage.weight0 = passage.get_passage_weight0()\n",
    "    passage.weight1 = passage.get_passage_weight1(rank_scale)\n",
    "    passage.weight2a = passage.get_passage_weight2(rank_scale)\n",
    "    passage.weight2b = passage.get_passage_weight2(rank_scale, div_all=False)\n",
    "    passage.weight3a = passage.get_passage_weight3(rank_scale)\n",
    "    passage.weight3b = passage.get_passage_weight3(rank_scale, div_all=False)\n",
    "    passage.weight3c = passage.get_passage_weight3(rank_scale, morph=True)\n",
    "\n",
    "    # Update the passage's word frequency and verb data.\n",
    "    for word in passage.words:\n",
    "        # Update the types and stems of verbs present. \n",
    "        if F.sp.v(word) == 'verb':\n",
    "            # if F.vt.v(word) not in c.easy_vtypes:\n",
    "            passage.verb_types_present.add(F.vt.v(word))\n",
    "            # if F.vt.v(word) not in c.easy_vstems:\n",
    "            passage.verb_stems_present.add(F.vs.v(word))\n",
    "        # Update the word_ranks_data dictionary with\n",
    "        # the words in each category.\n",
    "        for rank in rank_scale.keys():\n",
    "            lex_freq = F.freq_lex.v(word)\n",
    "            _range = rank_scale[rank]['range']\n",
    "            if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                passage.word_ranks_data[rank]['occ'] += 1\n",
    "                passage.word_ranks_data[rank]['words'].add(F.voc_lex_utf8.v(word))\n",
    "                    \n",
    "    # Update the passage's start and end reference.\n",
    "    start_ref = T.sectionFromNode(passage.verses[0])\n",
    "    end_ref = T.sectionFromNode(passage.verses[-1])\n",
    "    passage.start_ref = f\"{start_ref[0][:6]} {start_ref[1]}:{start_ref[2]}\"\n",
    "    passage.end_ref = f\"{end_ref[0][:6]} {end_ref[1]}:{end_ref[2]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterates over verses in the OT and combines them into passages. \n",
    "The function returns a list of Passage objects. \n",
    "\n",
    "rank_scale - a dictionary generated by Ranks().rank_scales()\n",
    "    For example:\n",
    "        rank_scales = Ranks().rank_scales(Ranks().all_ranks)[index]\n",
    "\n",
    "start_node - the verse node at which get_passages will begin. \n",
    "\n",
    "end_node - the verse node at which get_passages will finish executing.\n",
    "\n",
    "passage_size - the minimum words in a passage, unless a chapter is shorter\n",
    "than that (e.g., Psalm 117).\n",
    "\"\"\"\n",
    "def get_passages(\n",
    "    rank_scale, \n",
    "    start_node=0,\n",
    "    end_node=len(F.otype.s('verse')), \n",
    "    passage_size_min=100,\n",
    "    passage_size_max=4000\n",
    "    ):\n",
    "\n",
    "    # A list of all passages.\n",
    "    passages = []\n",
    "\n",
    "    # Initiate the id counter and instantiate the first passage.\n",
    "    passage_id = 1\n",
    "    passage = Passage(id=passage_id)\n",
    "\n",
    "    # Iterate through all verses in the OT. \n",
    "    for verse in F.otype.s('verse')[start_node:end_node]:\n",
    "\n",
    "        # Check if the string is a paragraph marker and if the paragraph is large enough.  \n",
    "        if len(passage.verses) > 1:\n",
    "            valid, add_verse = valid_passage(passage, verse, passage_size_min, passage_size_max)\n",
    "            if valid:\n",
    "\n",
    "                # We have reached the end of the passage so we update all of its attribute values.\n",
    "                if add_verse:\n",
    "                    passage.verses.append(verse)\n",
    "                update_passage_data(passage, rank_scale)\n",
    "                passages.append(passage)\n",
    "                # Begin a new passage. \n",
    "                passage_id += 1\n",
    "                passage = Passage(id=passage_id)\n",
    "\n",
    "                # The current verse is in a new chapter or book so we append it to the\n",
    "                # verses of the newly created passage as its start verse. \n",
    "                if not add_verse:\n",
    "                    passage.verses.append(verse)\n",
    "            # We haven't reached a new passage yet, so add the current verse to its list. \n",
    "            else:\n",
    "                passage.verses.append(verse)\n",
    "\n",
    "        # Add the first verse to the passage. \n",
    "        else:\n",
    "            passage.verses.append(verse)\n",
    "\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Lex Sample Clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weight(sample, rank_scale):\n",
    "    word_weights = {}\n",
    "    min_penalty = 1.7 # min penalty for rare words and proper nouns. \n",
    "    # Iterate over words in the passage.\n",
    "    words = L.d(sample, otype='word')\n",
    "    for word in words:\n",
    "        lex = F.voc_lex_utf8.v(word)\n",
    "        if lex not in Classify().stop_words:\n",
    "            # Add partial penalty for reocurring words. \n",
    "            if lex in word_weights.keys():\n",
    "                # Only gradually decrease penalty for rarer words. \n",
    "                # Decreases by 1 point per occurance. \n",
    "                word_weights[lex]['count'] += 1\n",
    "                if F.freq_lex.v(word) < 100:\n",
    "                    count = word_weights[lex]['count']\n",
    "                    penalty = word_weights[lex]['penalty']\n",
    "                    new_weight = penalty - count \n",
    "                    added_weight = new_weight if new_weight >= min_penalty else min_penalty\n",
    "                    word_weights[lex]['weight'] += added_weight\n",
    "                else:\n",
    "                    word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "            # Add full penalty for the first occurance. \n",
    "            else:\n",
    "                # Add word to hash table\n",
    "                word_weights[lex] = {'count':0, 'weight':0, 'penalty':0}\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    _range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        _penalty = rank_scale[rank]['weight']\n",
    "                        if F.sp.v(word) == 'nmpr' and _penalty > min_penalty: # proper noun\n",
    "                            word_weights[lex]['penalty'] = max((_penalty / 2), min_penalty)\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            word_weights[lex]['penalty'] = _penalty\n",
    "                word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                word_weights[lex]['count'] += 1\n",
    "\n",
    "    # Get the sum of all word weights. \n",
    "    total_weight = sum([w for w in [word_weights[k]['weight'] for k in word_weights.keys()]])\n",
    "    # Compare using all words as denominator vs. unique words.\n",
    "    total_weight = total_weight / len(words) \n",
    "    return round(total_weight, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get example sentences for each lexeme\n",
    "lex_samples = {}\n",
    "rank = LexRanks._10_ranks.get_rank_dict()\n",
    "for sentence in F.otype.s('sentence'):\n",
    "    s_weight = get_sample_weight(sentence, rank)\n",
    "    for w in L.d(sentence, otype='word'):\n",
    "        if F.freq_lex.v(w) < 5000:\n",
    "            lex = L.u(w, otype='lex')[0]\n",
    "            if lex in lex_samples:\n",
    "                if sentence not in [s[0] for s in lex_samples[lex]]:\n",
    "                    lex_samples[lex].append([s_weight, sentence])\n",
    "            else:\n",
    "                lex_samples[lex] = [[s_weight, sentence]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"lexId\":[], \"sentenceId\":[], \"sentenceWeight\":[]}\n",
    "for k, vals in lex_samples.items():\n",
    "    if len(vals) > 1:\n",
    "        for v in vals:\n",
    "            data['lexId'].append(k)\n",
    "            data['sentenceId'].append(v[1])\n",
    "            data['sentenceWeight'].append(v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('../../data_files/lex_sentences.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and Export Passage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to format printing output. \n",
    "# Takes a set of words as input and returns a string.\n",
    "def format_output(output):\n",
    "    # Sort the words by alphabetical order.\n",
    "    output = sorted(list(output))\n",
    "    formatted = ''\n",
    "    # Add spacing between the words until the\n",
    "    # last word is reached. \n",
    "    for item in output:\n",
    "        if item != output[-1]:\n",
    "            formatted += item + '  '\n",
    "        else:\n",
    "            formatted += item\n",
    "    # Return a string of the formatted words. \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the start verse node to get verses from 1 .. last verse. \n",
    "START_VS = F.otype.s('verse')[0] - 1\n",
    "\n",
    "def custom_df(passages):\n",
    "    data = {\n",
    "        'passageId': [],\n",
    "        'wordCount': [],\n",
    "        'weight': [],\n",
    "        'startVsId': [],\n",
    "        'endVsId': [],\n",
    "    }\n",
    "    for p in passages:\n",
    "        row = [p.id, p.word_count, p.weight3a, p.start_verse-START_VS, p.end_verse-START_VS]\n",
    "        for i, key in enumerate(data.keys()):\n",
    "            data[key].append(row[i])\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def all_ranks():\n",
    "    start = time.time()\n",
    "    rank_scales = LexRanks().all_ranks[6]\n",
    "    all_passage_rankings = []\n",
    "    for r_s in [rank_scales]:\n",
    "        rank_scale = r_s.get_rank_dict()\n",
    "        all_p = Passages(\n",
    "                passages= get_passages(\n",
    "                rank_scale, \n",
    "                # start_node=0,\n",
    "                # end_node=100, \n",
    "                # passage_size=100\n",
    "            ),\n",
    "            rank_scale=r_s)\n",
    "        all_passage_rankings.append(all_p)\n",
    "        print(r_s.name, \"complete\", time.time()-start)\n",
    "    return all_passage_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_ranks complete 31.837373971939087\n"
     ]
    }
   ],
   "source": [
    "all_rankings = all_ranks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = custom_df(all_rankings[0].weight_sorted3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT\n",
    "df.to_csv('../../data_files/passages.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
