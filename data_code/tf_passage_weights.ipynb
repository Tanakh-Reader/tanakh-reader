{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">TF-app:</b> <span title=\"rv2.4.0=#571428c995d309c43ff592f885a5ebb42a87be6f offline under ~/text-fabric-data\">~/text-fabric-data/annotation/app-bhsa/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv1.7.1=#f9f50075d169820af90524e89f573b54e0b1adb4 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/bhsa/tf/2021</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv2.0=#d74f7b52866927c316dfcaa2ac89eb51ea59c5dc offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/phono/tf/2021</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv2.0=#2fb9e94de8ffc07e9e515e3efa36e502f95bdcea offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/parallels/tf/2021</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 9.1.1\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "122 features found and 0 ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 9.1.1</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa TF-app\">app-bhsa v3</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/0_home\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Parallel Passages</b></summary><b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"~/text-fabric-data/etcbc/parallels/tf/2021/crossref.tf\">crossref</a></i></b><br></details><details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/book.tf\">book</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/book@am.tf\">book@ll</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/code.tf\">code</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/det.tf\">det</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/domain.tf\">domain</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/function.tf\">function</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/g_cons.tf\">g_cons</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/g_cons_utf8.tf\">g_cons_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/g_lex.tf\">g_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/g_lex_utf8.tf\">g_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/g_word.tf\">g_word</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/g_word_utf8.tf\">g_word_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/label.tf\">label</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/language.tf\">language</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/lex.tf\">lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/lex_utf8.tf\">lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/ls.tf\">ls</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/nametype.tf\">nametype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/nme.tf\">nme</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/number.tf\">number</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/pargr.tf\">pargr</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/pdp.tf\">pdp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/pfm.tf\">pfm</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/prs.tf\">prs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/prs_gn.tf\">prs_gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/prs_nu.tf\">prs_nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/prs_ps.tf\">prs_ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/ps.tf\">ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/qere_trailer.tf\">qere_trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/qere_trailer_utf8.tf\">qere_trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/qere_utf8.tf\">qere_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/rank_lex.tf\">rank_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/rela.tf\">rela</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/sp.tf\">sp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/st.tf\">st</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/tab.tf\">tab</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/trailer.tf\">trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/trailer_utf8.tf\">trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/txt.tf\">txt</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/typ.tf\">typ</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/uvf.tf\">uvf</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/vbe.tf\">vbe</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/vbs.tf\">vbs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/voc_lex.tf\">voc_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/voc_lex_utf8.tf\">voc_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/vs.tf\">vs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/vt.tf\">vt</a><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/mother.tf\">mother</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/omap@ll\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/omap@2017-2021.tf\">omap@ll</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"~/text-fabric-data/etcbc/bhsa/tf/2021/oslots.tf\">oslots</a></i></b><br></details><details><summary><b>Phonetic Transcriptions</b></summary><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/2021/phono.tf\">phono</a><br><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/2021/phono_trailer.tf\">phono_trailer</a><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".ubd {\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".col {\n",
       "   display: inline-block;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 2px;\n",
       "    margin: 2px;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -2px 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 2px 0px;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".tfsechead {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--tfsechead);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  10px 2px 2px 2px;\n",
       "    margin: 16px 2px 2px 2px;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 4px;\n",
       "    margin-left: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 4px;\n",
       "    margin-right: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -24px;\n",
       "    margin-left: 20px;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 6px;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 2px;\n",
       "    margin-left: 2px;\n",
       "    padding: 2px 2px;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 40px;\n",
       "\tpadding: 20px;\n",
       "\tborder: 2px solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--tfsechead:          hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         3px;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   2px;\n",
       "  --border-width0:      2px;\n",
       "  --border-width1:      3px;\n",
       "  --border-width2:      4px;\n",
       "  --border-width3:      6px;\n",
       "  --border-width4:      5px;\n",
       "  --border-width-plain: 2px;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 2px;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 4px ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 4px ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 6px;\n",
       "  padding: 4px;\n",
       "  margin: 4px;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>Text-Fabric API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"doc\">N F E L T S C TF</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tf.app import use\n",
    "\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranks:\n",
    "    ranks1 = ['Frequent', 'Common', 'Medium', 'Uncommon', 'Rare']\n",
    "    ranks2 = ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Scarce']\n",
    "    ranks3 = ['Frequent', 'Medium', 'Uncommon', 'Rare']\n",
    "    ranks4 = ['Frequent', 'Uncommon', 'Rare']\n",
    "    ranks5 = ['Frequent', 'Common', 'Infrequent', 'Rare', 'Scarce']\n",
    "    ranks6 = ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Rarer', 'Scarce', 'Scarcer', 'Scarcest']\n",
    "\n",
    "    # Using 2-elem lists is far faster than searching ranges. \n",
    "    # Rather than if i in range(), check if i > l[0] and <= l[1].\n",
    "    # Using this method scales runtime from ~0:04:30 to ~0:00:15.\n",
    "    ranges1 = [\n",
    "        [500, 51000],\n",
    "        [250, 500],\n",
    "        [150, 250],\n",
    "        [50, 150],\n",
    "        [1, 50],\n",
    "    ]\n",
    "    weights1 = [1, 2, 3, 5, 8]\n",
    "\n",
    "    ranges2 = [\n",
    "        [800, 51000],\n",
    "        [400, 800],\n",
    "        [200, 400],\n",
    "        [100, 200],\n",
    "        [50, 100],\n",
    "        [15, 50],\n",
    "        [1, 15],\n",
    "    ]\n",
    "    weights2 = [1, 1.1, 1.3, 1.7, 3, 5.5, 8.5]\n",
    "\n",
    "    ranges3 = [\n",
    "        [100, 51000],\n",
    "        [50, 100],\n",
    "        [10, 50],\n",
    "        [1, 10],\n",
    "    ]\n",
    "    weights3 = [1, 4, 5, 8]\n",
    "\n",
    "    ranges4 = [\n",
    "        [100, 51000],\n",
    "        [10, 100],\n",
    "        [1, 10],\n",
    "    ]\n",
    "    weights4 = [1, 3, 7]\n",
    "\n",
    "    ranges5 = [\n",
    "        [200, 51000],\n",
    "        [100, 200],\n",
    "        [50, 100],\n",
    "        [20, 50],\n",
    "        [1, 20],\n",
    "    ]\n",
    "    weights5 = [1, 1.5, 3, 5, 8]\n",
    "\n",
    "    ranges6 = [\n",
    "        [1000, 51000],\n",
    "        [400, 1000],\n",
    "        [200, 400],\n",
    "        [100, 200],\n",
    "        [50, 100],\n",
    "        [40, 50],\n",
    "        [30, 40],\n",
    "        [20, 30],\n",
    "        [10, 20],\n",
    "        [1, 10]\n",
    "    ]\n",
    "    weights6 = [1, 1.1, 1.3, 1.7, 3, 5.5, 7, 8, 9, 10]\n",
    "\n",
    "    all_ranks = [\n",
    "        ranks1, \n",
    "        ranks2, \n",
    "        ranks3, \n",
    "        ranks4, \n",
    "        ranks5,\n",
    "        ranks6,\n",
    "    ]\n",
    "    all_ranges = [\n",
    "        ranges1, \n",
    "        ranges2, \n",
    "        ranges3, \n",
    "        ranges4, \n",
    "        ranges5,\n",
    "        ranges6, \n",
    "    ]\n",
    "    all_weights = [\n",
    "        weights1, \n",
    "        weights2, \n",
    "        weights3, \n",
    "        weights4, \n",
    "        weights5,\n",
    "        weights6,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class that contains data to help assign difficulty weights to \n",
    "any portion of Hebrew text that has more than one word. \n",
    "\"\"\"\n",
    "class Classify:\n",
    "    \n",
    "    # Pass in your choice of ranges and weights from the Ranks class.\n",
    "    def __init__(self, ranks={}, ranges={}, weights={}):\n",
    "        self.ranks = ranks\n",
    "        self.ranges = ranges \n",
    "        self.weights = weights\n",
    "\n",
    "    \"\"\" \n",
    "    Notes on stop_words_types and other exclusion lists. \n",
    "\n",
    "    Most prepositions, articles, and conjunctions don't\n",
    "    add any meaningul weight to a text and could thus be exlcuded.\n",
    "    \n",
    "    Example use:\n",
    "    words = [w for w in passage if F.sp.v(w) not in stop_words_types]\n",
    "    \n",
    "    Note: the only Heb article is 'הַ' with 30,386 occurences. There are some \n",
    "    preps and conjs that have few occurences, so I recommend not using\n",
    "    stop_words_types when weighing passages and using stop_words instead.\n",
    "    \"\"\"\n",
    "    stop_words_types = ['prep', 'art', 'conj']\n",
    "    # Check if F.voc_lex_utf8.v(word) is in this list. If\n",
    "    # so it can be excluded since it occurs so often. \n",
    "    stop_words = ['אֵת', 'בְּ', 'לְ', 'הַ', 'וְ']\n",
    "    # If you take verb data into account when weighing a\n",
    "    # paragraph, these common types could be excluded. \n",
    "    easy_vtypes = ['perf', 'impf', 'wayq']\n",
    "    easy_vstems = ['qal', 'hif', 'nif', 'piel']\n",
    "\n",
    "    \"\"\"\n",
    "    rank_scale() returns a dict containing frequency ranking \n",
    "    buckets for a variety of lexical ranges.\n",
    "    \n",
    "    Example use of rank_scale():\n",
    "\n",
    "    r = Ranks()\n",
    "    rank_scale = Classify(r.ranks1, r.ranges1, r.weights1).rank_scale()\n",
    "    for rank in rank_scale.keys():\n",
    "        lex_freq = F.freq_lex.v(word)\n",
    "        range = rank_scale[rank]['range']\n",
    "        if lex_freq >= range[0] and lex_freq < range[1]:\n",
    "            total_weight += rank_scale[rank]['weight']\n",
    "    \"\"\"\n",
    "    # Create frequency ranking buckets for a variety of lexical ranges.\n",
    "    def rank_scale(self):\n",
    "        rank_scale = {}\n",
    "        for i in range(len(self.ranks)):\n",
    "            rank_scale[self.ranks[i]] = {\n",
    "                'range': self.ranges[i],\n",
    "                'weight': self.weights[i]\n",
    "            }\n",
    "        return rank_scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class that contains a Hebrew passage, consisting of paragraphs \n",
    "as marked by a petach (פ) or samech (ס) in the Masoretic Text. \n",
    "\"\"\"\n",
    "class Passage:\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.verses = [] # a list of verse node ints. \n",
    "        self.words = [] # a list of word node ints. \n",
    "        self.start_word = 0\n",
    "        self.end_word = 0\n",
    "        self.word_count = 0\n",
    "        self.weight1 = 0\n",
    "        self.weight2a = 0 # all words denom\n",
    "        self.weight2b = 0 # unique words\n",
    "        self.weight3a = 0 # all word denom\n",
    "        self.weight3b = 0\n",
    "        self.paragraph_markers = {'פ': 'open', 'ס': 'closed'}\n",
    "        self.verb_types_present = set()\n",
    "        self.verb_stems_present = set()\n",
    "        self.word_ranks_data = {}\n",
    "        self.start_ref = ''\n",
    "        self.end_ref = ''\n",
    "\n",
    "    # Reset the values of all the passage's mutable attributes.\n",
    "    def reset_values(self):\n",
    "        self.verses = []\n",
    "        self.words = []\n",
    "        self.start_word = 0\n",
    "        self.end_word = 0\n",
    "        self.word_count = 0\n",
    "        self.weight1 = 0\n",
    "        self.weight2a = 0\n",
    "        self.weight2b = 0 \n",
    "        self.weight3a = 0 \n",
    "        self.weight3b = 0\n",
    "        self.verb_types_present = set()\n",
    "        self.verb_stems_present = set()\n",
    "        self.word_ranks_data = {}\n",
    "        self.start_ref = ''\n",
    "        self.end_ref = ''\n",
    "\n",
    "    # Returns a list of all words present in the passage.\n",
    "    def get_all_words(self):\n",
    "        words = []\n",
    "        for verse in self.verses:\n",
    "            for word in L.i(verse, otype='word'):\n",
    "                words.append(word)\n",
    "        return words\n",
    "\n",
    "    # Returns a list of all words present in a specified verse in the passage.\n",
    "    def get_vs_words(verse):\n",
    "        verse_words = [w for w in L.i(verse, otype='word')]\n",
    "        return verse_words\n",
    "\n",
    "    # Returns a String of all the text in the passage.\n",
    "    def get_text(self):\n",
    "        return T.text(self.verses, fmt='text-orig-full')\n",
    "\n",
    "    \"\"\"\n",
    "    get_vs_weights() returns a dictionary mapping each verse node in \n",
    "    the passage to a weight. It takes rank_scale as input, an instance\n",
    "    of Classify(args).rank_scale() (see notes in Classify for instantiaion).\n",
    "    \"\"\"\n",
    "    def get_vs_weights(self, rank_scale):\n",
    "        # A dictionary mapping verse nodes to weights.\n",
    "        verse_weights = {}\n",
    "        # Iterate over verses in the passage.\n",
    "        for verse in self.verses:\n",
    "            verse_weight = 0\n",
    "            words = self.get_vs_words(verse)\n",
    "            # Add the scaled word weights to the verse's total weight.\n",
    "            for word in words:\n",
    "                if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                    for rank in rank_scale.keys():\n",
    "                        lex_freq = F.freq_lex.v(word)\n",
    "                        range = rank_scale[rank]['range']\n",
    "                        if lex_freq >= range[0] and lex_freq < range[1]:\n",
    "                            verse_weight += rank_scale[rank]['weight']\n",
    "            # Add the verse's weight to the dictionary at this verse's key. \n",
    "            verse_weight /= len(words)\n",
    "            verse_weights[verse] = round(verse_weight, 4)\n",
    "        \n",
    "        return verse_weights\n",
    "\n",
    "    def get_passage_weight1(self, rank_scale):\n",
    "        total_weight = 0\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= range[0] and lex_freq < range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                            total_weight += (rank_scale[rank]['weight']) / 2\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            total_weight += rank_scale[rank]['weight']\n",
    "        total_weight /= len(self.words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    # Only penalize once per lexical value.  \n",
    "    def get_passage_weight2(self, rank_scale, div_all=True):\n",
    "        total_weight = 0\n",
    "        unique_words = set()\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            lex = F.voc_lex_utf8.v(word)\n",
    "            if lex not in Classify().stop_words and lex not in unique_words:\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= range[0] and lex_freq < range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                            total_weight += (rank_scale[rank]['weight']) / 2\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            total_weight += rank_scale[rank]['weight']\n",
    "                unique_words.add(lex)\n",
    "        # Compare using all words as denominator vs. unique words.\n",
    "        if div_all:\n",
    "            total_weight /= len(self.words)\n",
    "        else:\n",
    "            total_weight /= len(unique_words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    # Decrease penalty for each occurance. \n",
    "    def get_passage_weight3(self, rank_scale, div_all=True):\n",
    "        word_weights = {}\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            lex = F.voc_lex_utf8.v(word)\n",
    "            if lex not in Classify().stop_words:\n",
    "                # Add partial penalty for reocurring words. \n",
    "                if lex in word_weights.keys():\n",
    "                    # Only gradually decrease penalty for rarer words. \n",
    "                    # Decreases by 1 point per occurance. \n",
    "                    word_weights[lex]['count'] += 1\n",
    "                    if F.freq_lex.v(word) < 100:\n",
    "                        count = word_weights[lex]['count']\n",
    "                        penalty = word_weights[lex]['penalty']\n",
    "                        new_weight = penalty - count \n",
    "                        added_weight = new_weight if new_weight >= 1 else 1\n",
    "                        word_weights[lex]['weight'] += added_weight\n",
    "                    else:\n",
    "                        word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                # Add full penalty for the first occurance. \n",
    "                else:\n",
    "                    # Add word to hash table\n",
    "                    word_weights[lex] = {'count':0, 'weight':0, 'penalty':0}\n",
    "                    # Iterate over the ranks present in the rank scale. \n",
    "                    for rank in rank_scale.keys():\n",
    "                        lex_freq = F.freq_lex.v(word)\n",
    "                        range = rank_scale[rank]['range']\n",
    "                        if lex_freq >= range[0] and lex_freq < range[1]:\n",
    "                            # Give a half penalty for proper nouns. \n",
    "                            if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                                word_weights[lex]['penalty'] = (rank_scale[rank]['weight']) / 2\n",
    "                            # Give a full penalty for other word types. \n",
    "                            else:\n",
    "                                word_weights[lex]['penalty'] = rank_scale[rank]['weight']\n",
    "                    word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                    word_weights[lex]['count'] += 1\n",
    "        # Get the sum of all word weights. \n",
    "        total_weight = sum([w for w in [word_weights[k]['weight'] for k in word_weights.keys()]])\n",
    "        # Compare using all words as denominator vs. unique words.\n",
    "        if div_all:\n",
    "            total_weight /= len(self.words)\n",
    "        else:\n",
    "            total_weight /= len(word_weights)\n",
    "        \n",
    "        return round(total_weight, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllPassages:\n",
    "    \n",
    "   def __init__(self, rank_scale={}):\n",
    "      self.rank_scale = rank_scale\n",
    "      self.order_sorted = []\n",
    "      self.word_count_sorted = []\n",
    "      self.weight_sorted1 = {}\n",
    "      self.weight_sorted2a = {}\n",
    "      self.weight_sorted2b = {}\n",
    "      self.weight_sorted3a = {}\n",
    "      self.weight_sorted3b = {}\n",
    "   \n",
    "   def word_count_sort(self):\n",
    "      return sorted(self.order_sorted, key=lambda p: p.word_count)\n",
    "\n",
    "   def weight_sort1(self):\n",
    "      sorted_list = sorted(self.order_sorted, key=lambda p: p.weight1)\n",
    "      return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "   def weight_sort2a(self):\n",
    "      sorted_list = sorted(self.order_sorted, key=lambda p: p.weight2a)\n",
    "      return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "   def weight_sort2b(self):\n",
    "      sorted_list = sorted(self.order_sorted, key=lambda p: p.weight2b)\n",
    "      return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "   def weight_sort3a(self):\n",
    "      sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3a)\n",
    "      return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "   def weight_sort3b(self):\n",
    "      sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3b)\n",
    "      return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "   def print_scale(self):\n",
    "      scale = self.rank_scale\n",
    "      output_text = \"\"\n",
    "      for rank in scale.keys():\n",
    "         range = scale[rank]['range'] \n",
    "         weight = scale[rank]['weight']\n",
    "         output = f\"w{weight} for {range[0]}-{range[1]} occ\"\n",
    "         output_text += f\"{rank}: {output}\\n\"\n",
    "      return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterates over all the verses in the OT and combines them\n",
    "into passages. The function returns a list of Passage objects. \n",
    "\n",
    "Takes rank_scale, an instance of Classify().rank_scale().\n",
    "For example:\n",
    "    r = Ranks()\n",
    "    rank_scale = Classify(r.ranks1, r.ranges1, r.weights1).rank_scale()\n",
    "\"\"\"\n",
    "# Set the minimal word requiremnt for a passage.\n",
    "# If a passage has less words than specified, it\n",
    "# will be comined with the following passage(s) \n",
    "# until len(passage.get_all_words()) >= passage_size.\n",
    "def get_passages(\n",
    "    rank_scale, \n",
    "    end_node=len(F.otype.s('verse')), \n",
    "    passage_size=100\n",
    "    ):\n",
    "\n",
    "    # A list of all passages.\n",
    "    passages = []\n",
    "\n",
    "    # Initiate the id counter and instantiate the first passage.\n",
    "    passage_id = 1\n",
    "    passage = Passage(id=passage_id)\n",
    "\n",
    "    # Iterate through all verses in the OT. \n",
    "    for verse in F.otype.s('verse')[:end_node]:\n",
    "\n",
    "        # Check if the string is a paragraph marker and if the paragraph is large enough.  \n",
    "        if len(passage.verses) > 1:\n",
    "            valid, add_verse = valid_passage(passage, verse, passage_size)\n",
    "            if valid:\n",
    "\n",
    "                # We have reached the end of the passage so we update all of its attribute values.\n",
    "                if add_verse:\n",
    "                    passage.verses.append(verse)\n",
    "                update_passage_data(passage, rank_scale)\n",
    "                passages.append(passage)\n",
    "\n",
    "                # Begin a new passage. \n",
    "                passage_id += 1\n",
    "                passage = Passage(id=passage_id)\n",
    "                # passage.reset_values()\n",
    "                if not add_verse:\n",
    "                    passage.verses.append(verse)\n",
    "            \n",
    "            else:\n",
    "                passage.verses.append(verse)\n",
    "\n",
    "        # If we haven't reached the end of the passage, simply add the present verse.\n",
    "        else:\n",
    "            passage.verses.append(verse)\n",
    "\n",
    "    return passages\n",
    "\n",
    "def valid_passage(passage, verse, passage_size):\n",
    "    is_valid = False\n",
    "    add_verse = True\n",
    "    # Get the string value at the end of the verse. \n",
    "    verse_ending = T.text(verse).split()[-1]\n",
    "    verse_book = L.u(verse, otype='book')\n",
    "    ps_119 = 427315 # node for Psalm 119\n",
    "    # Check if we've reached a new book, if yes, end the paragraph.\n",
    "    if L.u(passage.verses[-1], otype='book') != verse_book:\n",
    "        is_valid = True \n",
    "        add_verse = False\n",
    "    # Check if it's in Psalms\n",
    "    elif verse_book[0] == T.bookNode('Psalms'):\n",
    "        # If Psalm 119, split up into 8 verse sections\n",
    "        if L.u(verse, otype='chapter')[0] == ps_119:\n",
    "            if (verse-1) % 8 == 0:\n",
    "                is_valid = True \n",
    "                add_verse = False\n",
    "        # See if a new chapter is reached\n",
    "        elif L.u(verse, otype='chapter') != L.u(passage.verses[-1], otype='chapter'):\n",
    "            is_valid = True \n",
    "            add_verse = False\n",
    "    # Check if in the following books\n",
    "    elif verse_book[0] in [T.bookNode('Ruth'), T.bookNode('Jonah'), T.bookNode('Ecclesiastes')]:\n",
    "        if L.u(verse, otype='chapter') != L.u(passage.verses[-1], otype='chapter'):\n",
    "            is_valid = True \n",
    "            add_verse = False\n",
    "    # A normal paragraph\n",
    "    elif verse_ending in passage.paragraph_markers.keys() \\\n",
    "    and len(passage.get_all_words()) >= passage_size:\n",
    "        is_valid = True \n",
    "\n",
    "    return is_valid, add_verse\n",
    "\n",
    "def update_passage_data(passage, rank_scale):\n",
    "    passage.word_ranks_data = {k:{'occ':0, 'words':set()} for k in rank_scale.keys()}\n",
    "    passage.words = passage.get_all_words()\n",
    "\n",
    "    passage.start_word = passage.words[0]\n",
    "    passage.end_word = passage.words[-1]\n",
    "    passage.word_count = len(passage.words)\n",
    "\n",
    "    passage.weight1 = passage.get_passage_weight1(rank_scale)\n",
    "    passage.weight2a = passage.get_passage_weight2(rank_scale)\n",
    "    passage.weight2b = passage.get_passage_weight2(rank_scale, div_all=False)\n",
    "    passage.weight3a = passage.get_passage_weight3(rank_scale)\n",
    "    passage.weight3b = passage.get_passage_weight3(rank_scale, div_all=False)\n",
    "\n",
    "    # Update the passage's word frequency and verb data.\n",
    "    for word in passage.words:\n",
    "        # Update the types and stems of verbs present. \n",
    "        # if F.sp.v(word) == 'verb':\n",
    "        #     if F.vt.v(word) not in c.easy_vtypes:\n",
    "        #         passage.verb_types_present.add(F.vt.v(word))\n",
    "        #     if F.vt.v(word) not in c.easy_vstems:\n",
    "        #         passage.verb_stems_present.add(F.vs.v(word))\n",
    "        # Update the word_ranks_data dictionary with\n",
    "        # the words in each category.\n",
    "        for rank in rank_scale.keys():\n",
    "            lex_freq = F.freq_lex.v(word)\n",
    "            range = rank_scale[rank]['range']\n",
    "            if lex_freq >= range[0] and lex_freq < range[1]:\n",
    "                passage.word_ranks_data[rank]['occ'] += 1\n",
    "                passage.word_ranks_data[rank]['words'].add(F.voc_lex_utf8.v(word))\n",
    "                    \n",
    "    # Update the passage's start and end reference.\n",
    "    start_ref = T.sectionFromNode(passage.verses[0])\n",
    "    end_ref = T.sectionFromNode(passage.verses[-1])\n",
    "    passage.start_ref = f\"{start_ref[0][:6]} {start_ref[1]}:{start_ref[2]}\"\n",
    "    passage.end_ref = f\"{end_ref[0][:6]} {end_ref[1]}:{end_ref[2]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "426620\n"
     ]
    }
   ],
   "source": [
    "print(T.nodeFromSection(\"Ruth 1 1\"))\n",
    "# print(T.nodeFromHeading((('book', 'Ruth'), ('chapter', 1), ('verse', 1))))\n",
    "print(T.bookNode('Ruth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a dictionary of all passages in the OT,\n",
    "# # in the order of their occurences.\n",
    "# r = Ranks()\n",
    "# rank_scale = Classify(r.ranks2, r.ranges2, r.weights2).rank_scale()\n",
    "# # end_node = len(F.otype.s('verse'))\n",
    "# all_passages = AllPassages(rank_scale=rank_scale)\n",
    "# all_passages.order_sorted = get_passages(rank_scale)\n",
    "# all_passages.weight_sorted = all_passages.weight_sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to format printing output. \n",
    "# Takes a set of words as input and returns a string.\n",
    "def format_output(output):\n",
    "    # Sort the words by alphabetical order.\n",
    "    output = sorted(list(output))\n",
    "    formatted = ''\n",
    "    # Add spacing between the words until the\n",
    "    # last word is reached. \n",
    "    for item in output:\n",
    "        if item != output[-1]:\n",
    "            formatted += item + '  '\n",
    "        else:\n",
    "            formatted += item\n",
    "    # Return a string of the formatted words. \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output data to CSV\n",
    "# import csv\n",
    "\n",
    "# def to_csv(all_passages, _file):\n",
    "#     rank_scale = all_passages.rank_scale\n",
    "#     lim = -3\n",
    "#     rr = list(rank_scale.keys())[lim:] # rarest ranks\n",
    "#     with open(_file, mode='w', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file, delimiter=',')\n",
    "#         writer.writerow([\n",
    "#             all_passages.print_scale()\n",
    "#         ])\n",
    "#         writer.writerow([\n",
    "#             'Reference', \n",
    "#             'Weight', \n",
    "#             'Words',\n",
    "#             # 'Verb Types', \n",
    "#             # 'Verb Stems', \n",
    "#             f\"{rr[0]} ({rank_scale[rr[0]]['range'][0]}-{rank_scale[rr[0]]['range'][1]}):\",\n",
    "#             f\"{rr[1]} ({rank_scale[rr[1]]['range'][0]}-{rank_scale[rr[1]]['range'][1]}):\",\n",
    "#             f\"{rr[2]} ({rank_scale[rr[2]]['range'][0]}-{rank_scale[rr[2]]['range'][1]}):\",\n",
    "#         ])\n",
    "#         for p in all_passages.weight_sorted1:\n",
    "#             writer.writerow([\n",
    "#                 f\"{p.start_ref} - {p.end_ref}\", \n",
    "#                 p.weight1, \n",
    "#                 p.word_count,\n",
    "#                 # format_output(p.verb_types_present), \n",
    "#                 # format_output(p.verb_stems_present),\n",
    "#                 f\"{p.word_ranks_data[rr[0]]['occ']}  {format_output(p.word_ranks_data[rr[0]]['words'])}\",\n",
    "#                 f\"{p.word_ranks_data[rr[1]]['occ']}  {format_output(p.word_ranks_data[rr[1]]['words'])}\",\n",
    "#                 f\"{p.word_ranks_data[rr[2]]['occ']}  {format_output(p.word_ranks_data[rr[2]]['words'])}\"\n",
    "#             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data to CSV\n",
    "import csv\n",
    "\n",
    "def to_csv(all_passages, _file):\n",
    "    rank_scale = all_passages.rank_scale\n",
    "    lim = -5\n",
    "    rr = list(rank_scale.keys())[lim:] # rarest ranks\n",
    "    with open(_file, mode='w', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow([\n",
    "            all_passages.print_scale()\n",
    "        ])\n",
    "        headings = [\n",
    "            'Rank',\n",
    "            'Reference', \n",
    "            'Weight',\n",
    "            'Words',\n",
    "        ]\n",
    "        for i in range(len(rr)):\n",
    "            headings.append(\n",
    "                f\"{rr[i]} ({rank_scale[rr[i]]['range'][0]}-{rank_scale[rr[i]]['range'][1]}):\"\n",
    "            )        \n",
    "        writer.writerow(headings)\n",
    "        for p in all_passages.weight_sorted1:\n",
    "            row = [\n",
    "                all_passages.weight_sorted1[p]+1,\n",
    "                f\"{p.start_ref} - {p.end_ref}\", \n",
    "                p.weight1, \n",
    "                p.word_count,\n",
    "            ]\n",
    "            for i in range(len(rr)):\n",
    "                row.append(\n",
    "                    f\"{p.word_ranks_data[rr[i]]['occ']}  {format_output(p.word_ranks_data[rr[i]]['words'])}\",\n",
    "                )\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import xlsxwriter\n",
    "\n",
    "\n",
    "data = {\"1\":[\"xyz\",\"\"],\"2\":[\"abc\",\"def\"],\"3\":[\"zzz\",\"\"]}\n",
    "\n",
    "# Use an OrderedDict to maintain the order of the columns\n",
    "data = OrderedDict((k,data.get(k)) for k in sorted(data.keys()))\n",
    "\n",
    "# Open an Excel workbook\n",
    "workbook = xlsxwriter.Workbook('dict_to_excel.xlsx')\n",
    "\n",
    "# Set up a format\n",
    "book_format = workbook.add_format(properties={'bold': True, 'font_color': 'red'})\n",
    "\n",
    "# Create a sheet\n",
    "worksheet = workbook.add_worksheet('dict_data')\n",
    "\n",
    "# Write the headers\n",
    "for col_num, header in enumerate(data.keys()):\n",
    "    worksheet.write(0,col_num, int(header))\n",
    "\n",
    "# Save the data from the OrderedDict into the excel sheet\n",
    "for row_num,row_data in enumerate(zip(*data.values())):\n",
    "    for col_num, cell_data in enumerate(row_data):\n",
    "        if cell_data ==  \"xyz\":\n",
    "            worksheet.write(row_num+1, col_num, cell_data, book_format)\n",
    "        else:\n",
    "            worksheet.write(row_num+1, col_num, cell_data)\n",
    "\n",
    "# Close the workbook\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "files = [\n",
    "    '../output/rank1.csv',\n",
    "    '../output/rank2.csv',\n",
    "    '../output/rank3.csv',\n",
    "    '../output/rank4.csv',\n",
    "    '../output/rank5.csv',\n",
    "    '../output/rank6.csv'\n",
    "]\n",
    "\n",
    "def all_ranks():\n",
    "    start = time.time()\n",
    "    r = Ranks()\n",
    "    all_passage_rankings = []\n",
    "    # for i in range(len(r.all_ranks)):\n",
    "    for i in [5]:\n",
    "        rank_scale = Classify(r.all_ranks[i], r.all_ranges[i], r.all_weights[i]).rank_scale()\n",
    "        all_p = AllPassages(rank_scale=rank_scale)\n",
    "        all_p.order_sorted = get_passages(rank_scale, end_node=len(F.otype.s('verse')), passage_size=100)\n",
    "        all_p.word_count_sorted = all_p.word_count_sort()\n",
    "        all_p.weight_sorted1 = all_p.weight_sort1()\n",
    "        all_p.weight_sorted2a = all_p.weight_sort2a()\n",
    "        all_p.weight_sorted2b = all_p.weight_sort2b()\n",
    "        all_p.weight_sorted3a = all_p.weight_sort3a()\n",
    "        all_p.weight_sorted3b = all_p.weight_sort3b()\n",
    "        all_passage_rankings.append(all_p)\n",
    "        # file = files[i]\n",
    "        # to_csv(all_p, file)\n",
    "        end = time.time()\n",
    "        print(i+1, \"complete\", end-start)\n",
    "    return all_passage_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jm/7fp23jks6gn07fvs2ywycf3r0000gn/T/ipykernel_38428/3423707340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "print(len(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 complete 15.315574169158936\n"
     ]
    }
   ],
   "source": [
    "rankings = all_ranks()\n",
    "# to_csv(rankings[0], files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_df(passages, display_order):\n",
    "    rank_scale = passages.rank_scale\n",
    "    lim = -3\n",
    "    rr = list(rank_scale.keys())[lim:] # rarest ranks\n",
    "    df_cols = [\n",
    "        'Reference',\n",
    "        'Words',\n",
    "        'Weight', \n",
    "        'R2a',\n",
    "        'R2b',\n",
    "        'R3a',\n",
    "        'R3b',\n",
    "        f\"{rr[0]} ({rank_scale[rr[0]]['range'][0]}-{rank_scale[rr[0]]['range'][1]}):\",\n",
    "        f\"{rr[1]} ({rank_scale[rr[1]]['range'][0]}-{rank_scale[rr[1]]['range'][1]}):\",\n",
    "        f\"{rr[2]} ({rank_scale[rr[2]]['range'][0]}-{rank_scale[rr[2]]['range'][1]}):\",\n",
    "    ]\n",
    "    row_list = []\n",
    "    for p in display_order:\n",
    "        row_dict = {}\n",
    "        row = [\n",
    "            f\"{p.start_ref} - {p.end_ref}\", \n",
    "            p.word_count, \n",
    "            p.weight1,\n",
    "            # Go into the dict to get the ranking for that weight\n",
    "            passages.weight_sorted2a[p]+1,\n",
    "            passages.weight_sorted2b[p]+1,\n",
    "            passages.weight_sorted3a[p]+1,\n",
    "            passages.weight_sorted3b[p]+1,\n",
    "            f\"{p.word_ranks_data[rr[0]]['occ']}  {format_output(p.word_ranks_data[rr[0]]['words'])}\",\n",
    "            f\"{p.word_ranks_data[rr[1]]['occ']}  {format_output(p.word_ranks_data[rr[1]]['words'])}\",\n",
    "            f\"{p.word_ranks_data[rr[2]]['occ']}  {format_output(p.word_ranks_data[rr[2]]['words'])}\"\n",
    "        ]\n",
    "        for i in range(len(row)):\n",
    "            row_dict[df_cols[i]] = row[i]\n",
    "        row_list.append(row_dict)\n",
    "        # print(row_dict)\n",
    "    # print(row_list)\n",
    "    df = pd.DataFrame(row_list, columns=df_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(rankings[0], '../output/rank6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Words</th>\n",
       "      <th>Weight</th>\n",
       "      <th>R2a</th>\n",
       "      <th>R2b</th>\n",
       "      <th>R3a</th>\n",
       "      <th>R3b</th>\n",
       "      <th>Scarce (20-30):</th>\n",
       "      <th>Scarcer (10-20):</th>\n",
       "      <th>Scarcest (1-10):</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Psalms 117:1 - Psalms 117:2</td>\n",
       "      <td>20</td>\n",
       "      <td>2.2175</td>\n",
       "      <td>1570</td>\n",
       "      <td>1205</td>\n",
       "      <td>1481</td>\n",
       "      <td>589</td>\n",
       "      <td>1  גבר</td>\n",
       "      <td>0</td>\n",
       "      <td>2  אֻמָּה  שׁבח</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Psalms 134:1 - Psalms 134:3</td>\n",
       "      <td>33</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>427</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lament 5:19 - Lament 5:22</td>\n",
       "      <td>35</td>\n",
       "      <td>1.6800</td>\n",
       "      <td>1264</td>\n",
       "      <td>337</td>\n",
       "      <td>1099</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1  חדשׁ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hosea 14:9 - Hosea 14:10</td>\n",
       "      <td>38</td>\n",
       "      <td>1.8382</td>\n",
       "      <td>1417</td>\n",
       "      <td>738</td>\n",
       "      <td>1266</td>\n",
       "      <td>125</td>\n",
       "      <td>1  בְּרֹושׁ</td>\n",
       "      <td>3  עָצָב  רַעֲנָן  שׁור</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psalms 131:1 - Psalms 131:3</td>\n",
       "      <td>46</td>\n",
       "      <td>1.7543</td>\n",
       "      <td>1243</td>\n",
       "      <td>609</td>\n",
       "      <td>1170</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>1  שׁוה</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Psalms 119:137 - Psalms 119:144</td>\n",
       "      <td>51</td>\n",
       "      <td>2.5255</td>\n",
       "      <td>1624</td>\n",
       "      <td>1217</td>\n",
       "      <td>1563</td>\n",
       "      <td>620</td>\n",
       "      <td>2  פִּקּוּדִים  צָעִיר</td>\n",
       "      <td>1  צמת</td>\n",
       "      <td>2  מָצֹוק  שַׁעֲשׁוּעִים</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Psalms 150:1 - Psalms 150:6</td>\n",
       "      <td>51</td>\n",
       "      <td>3.3520</td>\n",
       "      <td>1737</td>\n",
       "      <td>1750</td>\n",
       "      <td>1745</td>\n",
       "      <td>1727</td>\n",
       "      <td>2  נְשָׁמָה  נֵבֶל</td>\n",
       "      <td>3  גֹּדֶל  רָקִיעַ  תֹּף</td>\n",
       "      <td>7  מֵן  מָחֹול  עוּגָב  צֶלְצֶלִים  שֶׁמַע  תֵּקַע</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Malach 3:22 - Malach 3:24</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0337</td>\n",
       "      <td>747</td>\n",
       "      <td>17</td>\n",
       "      <td>173</td>\n",
       "      <td>3</td>\n",
       "      <td>1  חֵרֶם</td>\n",
       "      <td>1  חֹרֵב</td>\n",
       "      <td>1  אֵלִיָּה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Psalms 93:1 - Psalms 93:5</td>\n",
       "      <td>52</td>\n",
       "      <td>2.6327</td>\n",
       "      <td>1631</td>\n",
       "      <td>1318</td>\n",
       "      <td>1646</td>\n",
       "      <td>957</td>\n",
       "      <td>2  אַדִּיר</td>\n",
       "      <td>1  אזר</td>\n",
       "      <td>4  גֵּאוּת  דֳּכִי  מִשְׁבָּר  נאה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Psalms 119:25 - Psalms 119:32</td>\n",
       "      <td>52</td>\n",
       "      <td>2.2615</td>\n",
       "      <td>1562</td>\n",
       "      <td>1036</td>\n",
       "      <td>1484</td>\n",
       "      <td>445</td>\n",
       "      <td>3  פִּקּוּדִים  רחב  שׂיח</td>\n",
       "      <td>0</td>\n",
       "      <td>3  דלף  שׁוה  תּוּגָה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Psalms 123:1 - Psalms 123:4</td>\n",
       "      <td>53</td>\n",
       "      <td>2.2943</td>\n",
       "      <td>1419</td>\n",
       "      <td>1250</td>\n",
       "      <td>1451</td>\n",
       "      <td>967</td>\n",
       "      <td>0</td>\n",
       "      <td>4  בּוּז  ינה  שַׁאֲנָן</td>\n",
       "      <td>3  גְּבֶרֶת  גֵּאֶה  לַעַג</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Psalms 133:1 - Psalms 133:3</td>\n",
       "      <td>53</td>\n",
       "      <td>2.0717</td>\n",
       "      <td>1306</td>\n",
       "      <td>625</td>\n",
       "      <td>1284</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>5  זָקָן  חֶרְמֹון  מַד  נָעִים</td>\n",
       "      <td>1  טֹוב</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Psalms 100:1 - Psalms 100:5</td>\n",
       "      <td>56</td>\n",
       "      <td>1.5607</td>\n",
       "      <td>1171</td>\n",
       "      <td>497</td>\n",
       "      <td>1005</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>1  מַרְעִית</td>\n",
       "      <td>1  רְנָנָה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Psalms 119:1 - Psalms 119:8</td>\n",
       "      <td>57</td>\n",
       "      <td>1.8719</td>\n",
       "      <td>1330</td>\n",
       "      <td>610</td>\n",
       "      <td>1264</td>\n",
       "      <td>194</td>\n",
       "      <td>1  פִּקּוּדִים</td>\n",
       "      <td>1  יֹשֶׁר</td>\n",
       "      <td>1  אַחֲלַי</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Psalms 119:9 - Psalms 119:16</td>\n",
       "      <td>57</td>\n",
       "      <td>2.2368</td>\n",
       "      <td>1577</td>\n",
       "      <td>1115</td>\n",
       "      <td>1474</td>\n",
       "      <td>454</td>\n",
       "      <td>5  הֹון  פִּקּוּדִים  שׁגה  שׂושׂ  שׂיח</td>\n",
       "      <td>0</td>\n",
       "      <td>2  זכה  שׁעע</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Psalms 119:49 - Psalms 119:56</td>\n",
       "      <td>57</td>\n",
       "      <td>2.3018</td>\n",
       "      <td>1594</td>\n",
       "      <td>1208</td>\n",
       "      <td>1525</td>\n",
       "      <td>623</td>\n",
       "      <td>1  פִּקּוּדִים</td>\n",
       "      <td>3  זֵד  ליץ  מְגוּרִים</td>\n",
       "      <td>3  זַלְעָפָה  זָמִיר  נֶחָמָה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Psalms 119:145 - Psalms 119:152</td>\n",
       "      <td>58</td>\n",
       "      <td>2.2603</td>\n",
       "      <td>1537</td>\n",
       "      <td>979</td>\n",
       "      <td>1469</td>\n",
       "      <td>415</td>\n",
       "      <td>5  זִמָּה  קדם  שׁוע  שׂיח</td>\n",
       "      <td>1  נֶשֶׁף</td>\n",
       "      <td>1  אַשְׁמוּרָה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Psalms 119:169 - Psalms 119:176</td>\n",
       "      <td>59</td>\n",
       "      <td>2.4237</td>\n",
       "      <td>1596</td>\n",
       "      <td>1065</td>\n",
       "      <td>1554</td>\n",
       "      <td>522</td>\n",
       "      <td>2  פִּקּוּדִים  תְּחִנָּה</td>\n",
       "      <td>2  נבע  ענה</td>\n",
       "      <td>2  שַׁעֲשׁוּעִים  תאב</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Psalms 119:33 - Psalms 119:40</td>\n",
       "      <td>60</td>\n",
       "      <td>2.1100</td>\n",
       "      <td>1515</td>\n",
       "      <td>1002</td>\n",
       "      <td>1414</td>\n",
       "      <td>364</td>\n",
       "      <td>2  בֶּצַע  פִּקּוּדִים</td>\n",
       "      <td>1  עֵקֶב</td>\n",
       "      <td>3  יגר  נָתִיב  תאב</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Psalms 119:57 - Psalms 119:64</td>\n",
       "      <td>60</td>\n",
       "      <td>1.9767</td>\n",
       "      <td>1496</td>\n",
       "      <td>745</td>\n",
       "      <td>1350</td>\n",
       "      <td>127</td>\n",
       "      <td>1  פִּקּוּדִים</td>\n",
       "      <td>2  חָבֵר  חושׁ</td>\n",
       "      <td>2  חֲצֹות  מָהַהּ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "passages = rankings[0]\n",
    "display_order = passages.word_count_sorted\n",
    "df = create_df(passages, display_order)\n",
    "display(HTML(\n",
    "    df.head(20).to_html(index=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/test.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Ruth\n",
    "# Proverbs 10-13 & 14-19\n",
    "# Eccl 3-9 & 9-11\n",
    "# Gen 28:10 - 32:10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight metrics comparison\n",
    "#### 5-weight ranking:\n",
    "```\n",
    "scale = {\n",
    "    'frequent': list(range(500, 50000)),    1\n",
    "    'common':   list(range(250, 500)),      2\n",
    "    'medium':   list(range(150, 250)),      3\n",
    "    'uncommon': list(range(50, 150)),       5\n",
    "    'rare':     list(range(1, 50))          8\n",
    "}\n",
    "```\n",
    "#### 7-weight ranking\n",
    "```\n",
    "rank_scale = {\n",
    "    'abundant': list(range(1000, 50000)),   1\n",
    "    'frequent': list(range(500, 1000)),     1.5\n",
    "    'common':   list(range(300, 500)),      2.25\n",
    "    'average':  list(range(175, 300)),      3.5\n",
    "    'uncommon': list(range(75, 175)),       5\n",
    "    'rare':     list(range(35, 75)),        7.5\n",
    "    'scarce':   list(range(1, 35))          11\n",
    "}\n",
    "```\n",
    "\n",
    "#### 14 easiest passages\n",
    "|    | 5-rank          | weight | U, R  | 7-rank          | weight | U, R, S  |\n",
    "|----|-----------------|--------|-------|-----------------|--------|----------|\n",
    "| 1  | Jer 26:7-15     | 1.3418 | 11, 5 | Jer 26:7-15     | 2.0767 | 9, 7, 1  |\n",
    "| 2  | Ezek 37:11-28   | 1.3449 | 14, 7 | 2 Sam 7:18-29   | 2.1226 | 6, 4, 5  |\n",
    "| 3  | 2 Sam 7:18-29   | 1.386  | 10, 6 | Deut 18:14-22   | 2.1545 | 7, 0, 7  |\n",
    "| 4  | 2 Kings 8:1-6   | 1.4181 | 8, 3  | 1 Chr 17:3-15   | 2.1731 | 5, 7, 3  |\n",
    "| 5  | 1 Chr 17:3-15   | 1.4187 | 6, 8  | Ezek 37:11-28   | 2.2009 | 16, 3, 5 |\n",
    "| 6  | Ex 33:12-23     | 1.4191 | 4, 7  | Jer 35:12-18    | 2.2769 | 6, 3, 3  |\n",
    "| 7  | Josh 21:43-22:6 | 1.4211 | 11, 2 | 1 Chr 17:16-27  | 2.2945 | 5, 5, 6  |\n",
    "| 8  | Deut 18:14-22   | 1.4313 | 4, 7  | Josh 21:43-22:6 | 2.2952 | 8, 2, 2  |\n",
    "| 9  | Ex 6:2-12       | 1.4399 | 8, 9  | 1 Kings 8:12-21 | 2.3107 | 8, 1, 4  |\n",
    "| 10 | Jer 7:20-28     | 1.4461 | 7, 8  | Deut 31:1-13    | 2.3617 | 6, 10, 4 |\n",
    "| 11 | Ex 6:29-7:13    | 1.4465 | 7, 9  | Ex 6:2-12       | 2.3786 | 7, 4, 6  |\n",
    "| 12 | Jer 16:9-15     | 1.4568 | 6, 6  | Num 8:23-9:8    | 2.422  | 11, 3, 2 |\n",
    "| 13 | 1 Sam 12:6-17   | 1.4605 | 5, 14 | Jer 44:24-29    | 2.4236 | 6, 5, 6  |\n",
    "| 14 | Deut 31:1-13    | 1.4703 | 9, 9  | Ex 6:29-7:13    | 2.4237 | 5, 4, 6  |\n",
    "\n",
    "Note: the counts for U (uncommon), R (rare), and S (scarce) words only include *unique* occurences. This means, for example, that if the scarce word תנין occured 5 times in the passage, it would only be counted once in the list of scarce words. \n",
    "\n",
    "#### TODO\n",
    "- Experiment with different paragraph size metrics.\n",
    "- Fine-tune the weight system.\n",
    "    - Currently a passage is weighted by the lexical frequency of each (non-stop) word present. However, since in a paragraph's *word_ranks_data* I only include unique words, an infrequent word that occurs many times in a paragraph could unnecesarily penalize the weight. For example, if a word that only occurs 15 times has 8 of its occurences in the present paragraph, it would add a weight of 88 with the 7-rank system, even though the reader will become familiar with it by the end of the paragraph. \n",
    "    - To troubleshoot this, we could either only increase the weight of a passage according to each unique word, or decrease the weight each consecutive time it shows up in the paragraph. \n",
    "- Find better data for lexical forms.\n",
    "    - The BHSA dataset doesn't follow the lexical forms in Strongs closely enough. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
